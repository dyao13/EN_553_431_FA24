\documentclass[titlepage]{article}

\usepackage{preamble}

\begin{document}

\maketitle

\tableofcontents

\newpage \newsection{Introduction.}

\subsection{Introduction.} Introduction.

\subsection{Remark.} The following notes follow the material presented in EN.553.431 Honors Mathematical Statistics taught by Professor Avanti Athreya during the semester of Fall 2024 at The Johns Hopkins University. The content of lectures is presented along with selected homework exercises.

\subsection{Notation.} Rice shall refer to 'Mathematical Statistics and Data Analysis' 3rd edition (US) by John A. Rice.

\subsection{Notation.} The following abbreviations shall be overserved.
\begin{enumerate}
\item The term 'rv' shall denote 'random variable'.
\item The term 'pmf' shall denote 'probability mass function'.
\item The term 'pdf' shall denote 'probability density function'.
\item The term 'cdf' shall denote 'cumulative density function'.
\item The term 'iid' shall denote 'independent and identically distributed'.
\item The term 'mom' shall denote 'method of moments'.
\item The term 'mle' shall denote 'maximum likelihood estimator'.
\item The term 'df' shall denote degrees of freedom.
\item The term 'glr' shall denote 'generalized likelihood ratio'.
\end{enumerate}

\subsection{Notation} A finite population sample shall be as follows. We are given a finite bivariate population of $N$ distinct objects, and associated to each object $k$ is a pair of measurements $(x_k, y_k)$. Suppose our population of measurements is represented by $\{(x_1, y_1), \ldots, (x_N, y_N)\}$. We assume $N > 1$. Let $\tau_x$ and $\tau_y$ be the population totals of the $x$- and $y$-measurements, respectively; let $\mu_x$ and $\mu_y$ be the population means of the $x$- and $y$-measurements, respectively; let $\sigma_x^2$ and $\sigma_y^2$ denote the population variances of the $x$- and $y$-measurements, respectively. Let $\sigma_{xy}$ denote the population covariance.
The population 3rd and 4th moments, $\mu_3(x)$ and $\mu_4(x)$, respectively, of the $x$-values are
$$\mu_3(x) = \frac{1}{N} \sum_{k=1}^{N} x_k^3, \quad \mu_4(x) = \frac{1}{N} \sum_{k=1}^{N} x_k^4$$
Similarly, the population 3rd and 4th moments are, respectively, $\mu_3(y)$ and $\mu_4(y)$.
Let $\sigma_{x^2y^2}$ denote
$$\sigma_{x^2y^2} = \left( \frac{1}{N} \sum_{k=1}^{N} x_k^2 y_k^2 \right) - (\sigma_x^2 + \mu_x^2)(\sigma_y^2 + \mu_y^2)$$
Let $M_x$ and $M_y$ represent the population maximum of the $x$- and $y$-values, respectively, so that $M_x$ and $M_y$ are defined by
$$M_x = \max\{x_k : 1 \leq k \leq N\}, \quad M_y = \max\{y_k : 1 \leq k \leq N\}$$
Let $m_x$ and $m_y$ denote the population minimum of the $x$- and $y$-measurements, respectively, so that
$$m_x = \min\{x_k : 1 \leq k \leq N\}, \quad m_y = \min\{y_k : 1 \leq k \leq N\}$$
All sample sizes $n$ satisfy $n \geq 1$, and in some cases, we specify if $n > 1$ or we give an explicit value for $n$. In what follows below, $\bar{X}$ denotes the sample mean of the $x$-measurements in the sample, and $\bar{Y}$ denotes the sample mean of the $y$-measurements in the sample. The letter $\E$ represents expected value; $\Var$ represents variance; and $\Cov$ represents covariance.

\newpage \newsection{Probability.}

\subsection{Introduction.} Probability.

\subsection{Remark.} This course EN.553.431 Honors Mathematical Statistics is taken after the prerequisite EN.553.420 Probability or EN.553.420 Honors Probability. The following notes shall be brief.

\subsection{Definition.} If $X$ and $Y$ are rvs with respective pdfs $f_{x}$ and $f_{y}$ and joint pdf $f_{xy}$, then the conditional pdf of $Y$ given $X = x$ is
$$f_{y|x}(y|x) = \frac{f_{xy}(x, y)}{f_{x}(x)}.$$

\subsection{Definition.} If $X$ and $Y$ are rvs with respective pdfs $f_{x}$ and $f_{y}$, then $X$ and $Y$ are independent if and only if the joint pdf 
$$f_{xy}(x, y) = f_{x}(x)f_{y}(y).$$
This implies that the conditional pdf of $Y$ given $X = x$ is
$$f_{y|x}(y|x) = f_{y}(y),$$
i.e., the marginal pdf of $Y$.

\subsection{Definition.} If $X$ and $Y$ are rvs with joint pdf $f_{xy}$, then $X$ and $Y$ are exchangeable if and only if 
$$f_{xy}(x, y) = f_{xy}(y, x).$$
That is, the joint pdf is symmetric under permutations of its arguments.

\subsection{Theorem.} (Markov's Inequality.) Let $X$ be a nonnegative rv. Then, for any $a > 0$, 
$$P(X \geq a) \leq \frac{\E(X)}{a}.$$

\subsection{Proof.} (Continued.) Observe that $X$ may be written as 
$$X = XI_{\{X < a\}} + XI_{\{X \geq a\}}.$$
Then, since $X \geq 0$ and expectation is monotonic, we have that 
\begin{align*}
          \E(X) &\leq \E(XI_{\{X \geq a\}}) \\
                &\leq aP(X \geq a) \\
    P(X \geq a) &\leq \frac{\E(X)}{a}.
\end{align*}

\subsection{Theorem.} (Chebyshev's Inequality.) Let $X$ be a rv with finite mean $\mu$ and finite variance $\sigma^{2}$. Then, for any $\delta > 0$, 
$$P(|X - \mu| \geq \delta) \leq \frac{\sigma^{2}}{\delta^{2}}.$$

\subsection{Proof.} (Continued.) From Markov's Inequality, take 
$$X = (X - \mu)^{2}$$
and 
$$\E\left(X - \mu\right)^{2} = \sigma^{2}$$
to obtain Chebyshev's Inequality.

\subsection{Theorem.} (Cauchy-Schwarz Inequality.) Let $X$ and $Y$ be rvs with finite second moments. Then,
$$|\Cov(X, Y)| \leq \sqrt{\Var(X)\Var(Y)}.$$

\subsection{Theorem.} (Jensen's Inequality.) Let $f$ be a convex function and $X$ be a rv. Then,
$$f(\E(X)) \leq \E(f(X)).$$
If $f$ is strictly convex, then the inequality is strict if $X$ is not degenerate.

\subsection{Definition.} For two random $k$-vectors $X$ and $Y$, the covariance matrix is a matrix $\Sigma \in \mathbb{R}^{k \times k}$ such that
$$(\Sigma)_{ij} = \Cov(X_{i}, Y_{j}).$$

\subsection{Theorem.} Suppose that $X,Y$ are random $k$-vectors and that $A, B \in \mathbb{R}^{m \times k}$. Then, the covariance matrix between $AX$ and $BY$ is an $m \times m$ matrix given by 
$$\Cov(AX, BY) = A\Sigma B^{T}.$$

\subsection{Definition.} For $Z_{1}, Z_{2} \sim \text{N}(0, 1)$ iid, the rvs
\begin{align*}
    X &= \sigma_{x}Z_{1} + \mu_{x} \\
    Y &= \sigma_{y}(\rho Z_{1} + \sqrt{1 - \rho^{2}}Z_{2}) + \mu_{y}
\end{align*}
form a bivariate normal distribution with means $(\mu_{x}, \mu_{y})$, variances $(\sigma_{x}^{2}, \sigma_{y}^{2})$, and covariance $\sigma_{xy} = \rho\sigma_{x}\sigma_{y}$. We may write the joint pdf of $X$ and $Y$ in matrix form as 
$$f_{x_{1},x_{2}}(x_{1}, x_{2}) = \frac{1}{2\pi\sigma_{1}\sigma_{2}\sqrt{1-\rho^{2}}}\exp(-A(x_{1}, x_{2})/2)$$
where 
$$A(x_{1}, x_{2}) = (x-\mu)^{T}\Sigma^{-1}(x-\mu)$$
where 
$$\Sigma = \begin{bmatrix} \sigma_{x}^{2} & \rho\sigma_{x}\sigma_{y} \\ \rho\sigma_{x}\sigma_{y} & \sigma_{y}^{2} \end{bmatrix}$$
is the covariance matrix.

\subsection{Theorem.} For $X,Y$ bivariate normal and $D \in \mathbb{R}^{2 \times 2}$, the covariance matrix of 
$$\begin{bmatrix} U \\ V \end{bmatrix} = D\begin{bmatrix} X \\ Y \end{bmatrix}$$
where 
$$D = \begin{bmatrix} a_{1} & a_{2} \\ b_{1} & b_{2} \end{bmatrix}$$
is 
$$\Sigma_{uv} = D\Sigma_{xy}D^{T}.$$
From the method of Jacobians, we have that the bivariate normal pdf of $U,V$ is therefore 
$$f_{uv}(u, v) = \frac{1}{2\pi\sqrt{\det(\Sigma_{uv})}}\exp\left(A(u, v)/2\right)$$
where 
$$A(u, v) = (u - \mu_{u})^{T}\Sigma_{uv}^{-1}(u - \mu_{u}).$$
and
$$\mu_{u} = D\mu_{x}.$$

\newpage \newsection{Finite Population Samples.}

\subsection{Introduction.} Finite Population Samples.

\subsection{Definition.} Let $k = 1, 2, \ldots, N$ index the objects in a finite population of size $N$. The objects $z_{k}$ are represented as measurement-index tuples 
$$z_{k} = (x_{k}, k)$$
where $x_{k} \in \mathbb{R}^{d}$ is the numerical d-tuple of measurements for object $k$.

\subsection{Definition.} For an object $z_{k}$ in a finite population, let the operators 
\begin{align*}
                   \pi(z_{k}) &= x_{k} \\
               \pi_{i}(z_{k}) &= x_{k,i} \\
    \pi_{\text{index}}(z_{k}) &= k
\end{align*}
where $\pi$ is the projection operator, $\pi_{i}$ is the $i$th component projection operator, and $\pi_{\text{index}}$ is the index projection operator.

\subsection{Definition.} For a bivariate population of size $N$ such that 
$$z_{k} = (x_{k}, y_{k}),$$
the population parameters are 
\begin{align*}
           \mu_{x} &= \frac{1}{N}\sum_{k=1}^{N}x_{k} \\
    \sigma_{x}^{2} &= \frac{1}{N}\sum_{k=1}^{N}(x_{k} - \mu_{x})^{2} \\
       \sigma_{xy} &= \frac{1}{N}\sum_{k=1}^{N}(x_{k} - \mu_{x})(y_{k} - \mu_{y}) \\
\end{align*}
along with the corresponding parameters for the $y$-measurements. Note the similarities between $\mu_{x}$ and expectation, $\sigma_{x}^{2}$ and variance, and $\sigma_{xy}$ and covariance. We often refer to these respective parameters as the population mean, the population variance, and the population covariance.

\subsection{Definition.} The population total is 
\begin{align*}
    \tau_{x} &= \sum_{k=1}^{N}x_{k} \\
             &= N\mu_{x}.
\end{align*}

\subsection{Example.} An easier way to compute the population variance is
\begin{align*}
    \sigma_{x}^{2} &= \frac{1}{N}\sum_{k=1}^{N}(x_{k} - \mu)^{2} \\
                   &= \frac{1}{N}\sum_{k=1}^{N}\left(x_{k}^{2} - 2x_{k}\mu + \mu^{2}\right) \\
                   &= \frac{1}{N}\sum_{k=1}^{N}x_{k}^{2} - 2\mu\frac{1}{N}\sum_{k=1}^{N}x_{k} + \mu^{2} \\
                   &= \frac{1}{N}\sum_{k=1}^{N}x_{k}^{2} -\mu^{2},
\end{align*}
i.e., the second population moment minus the square of the population mean. Similarly, the population covariance is 
$$\sigma_{xy} = \frac{1}{N}\sum_{k=1}^{N}x_{k}y_{k} - \mu_{x}\mu_{y}.$$

\subsection{Definition.} A uniform sample of size $n$ is a sample of $n$ objects drawn from a finite population of size $N$ such that each object has an equal probability of being selected. We consider a sample as a collection of random objects 
$$\{Z_{1}, Z_{2}, \ldots Z_{n}.\}$$
We consider the special cases:
\begin{enumerate}
\item[(1)] \textit{With Replacement.} An object may be drawn from the population multiple times. In this case, the sample points are independent.
\item[(2)] \textit{Without Replacement.} An object may be drawn at most once from the population. In this case, the sample points are not independent. Note that a sample without replacement can only have sample size $n \leq N$.
\end{enumerate}

\subsection{Lemma.} A uniform sample with replacement of size $n$ produces iid sample points. A uniform sample without replacement of size $n$ produces identically distributed but not independent sample points. In either case, the probability that the $i$th sample point is $z_{k}$ is
$$P(X_{i} = z_{k}) = \frac{1}{N}$$
for all $i = 1, 2, \ldots, n$ and $k = 1, 2, \ldots, N$ and the sample points are exchangeable.

\subsection{Definition.} For a finite bivariate population of size $N$ with objects 
$$z_{k} = (x_{k}, y_{k}),$$
the (distinct) numerical values of the $x$- and $y$-measurements are 
$$\xi_{l} \text{ and } \eta_{r}$$
for $l = 1, 2, \ldots, L$ and $r = 1, 2, \ldots, R$ where $L$ and $R$ are the respective number of distinct $x$- and $y$-measurements. The probability that the $i$th sample point has $X_{i} = \xi_{l}$ is
$$P(X_{i} = \xi_{l}) = \frac{n_{l}}{N}$$
and similarly for the $y$-measurements.

\subsection{Remark.} Observe the distinction between $x_{k}$ and $\xi_{l}$. $x_{k}$ is the $x$-measurement, which need not be distinct, for the $k$th object. That is 
$$X_{1} = x_{k} \text{ iff } Z_{1} = z_{k}.$$
On the other hand, $\xi_{l}$ is a distinct $x$-measurement. That is 
$$X_{1} = \xi_{l} \text{ iff } Z_{1} \in \{Z_{k} : x_{k} = \xi_{l}\}.$$

\subsection{Example.} If $Z_{i}$ is the $i$th object in a uniform sample, then 
\begin{align*}
    \E(X) &= \sum_{l=1}^{L}\xi_{l}P(X = \xi_{l}) \\
           &= \sum_{l=1}^{L}\xi_{l}\frac{n_{l}}{N} \\
           &= \mu_{x}.
\end{align*}

\subsection{Definition.} For a sample of size $n$, the sample mean is defined as 
$$\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_{i}.$$

\subsection{Theorem.} For a uniform sample with replacement, the sample mean has the following properties:
\begin{enumerate}
\item[(1)] The expectation is 
$$\E(\bar{X}) = \mu_{x}.$$
\item[(2)] The variance is
$$\Var(\bar{X}) = \frac{1}{n}\sigma^{2}.$$
\end{enumerate}

\subsection{Proof.} (Continued.) Observe that our sample points are iid. We have that 
\begin{align*}
    \E(\bar{X}) &= \E\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right) \\
                &= \frac{1}{n}\sum_{i=1}^{n}\E(X_{i}) \\
                &= \mu_{x}
\end{align*}
since expectation is linear. We also have that 
\begin{align*}
    \Var(\bar{X}) &= \Var\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right) \\
                  &= \frac{1}{n^{2}}\sum_{i=1}^{n}\Var(X_{i}) \\
                  &= \frac{1}{n}\sigma^{2}
\end{align*}
since variance is bilinear.

\subsection{Theorem.} If we sample uniformly with replacement, the covariance between $X_{i}$ and $X_{j}$ is 
$$\Cov(X_{i}, X_{j}) = 0.$$
If we sample uniformly without replacement, the covariance between $X_{i}$ and $X_{j}$ is
$$\Cov(X_{i}, X_{j}) = -\frac{\sigma^{2}}{N-1}.$$

\subsection{Proof.} (Continued.) If we sample uniformly with replacement, then the sample points are iid, so the covariance is zero. If we sample uniformly without replacement, then the sample points are not independent. Suppose that our sample is of size $n = N$. Then, 
$$\Var(\bar{X}) = 0$$
because we exhaust the population. We have that 
\begin{align*}
                 0 &= \Var(\bar{X}) \\
                   &= \frac{1}{N^{2}}\Cov\left(\sum_{i=1}^{N}X_{i}, \sum_{j=1}^{N}X_{j}\right) \\
                   &= \frac{1}{N^{2}}\left(\sum_{i=1}^{N}\Var(X_{i}) + \sum\sum_{i \neq j}\Cov(X_{i}, X_{j})\right) \\
                   &= \frac{1}{N^{2}}\left(N\sigma^{2} + N(N-1)\Cov(X_{i}, X_{j})\right) \\
                   &= \frac{1}{N}\sigma^{2} + \frac{N-1}{N}\Cov(X_{i}, X_{j}) \\
\Cov(X_{i}, X_{j}) &= -\frac{\sigma^{2}}{N-1}.
\end{align*}

\subsection{Theorem.} If we sample from a bivariate population with replacement, the covariance between $X_{i}$ and $Y_{j}$ is
$$\Cov(X_{i}, Y_{j}) = 0.$$
If we sample from a bivariate population without replacement, the covariance between $X_{i}$ and $Y_{j}$ is
$$\Cov(X_{i}, Y_{j}) = -\frac{\sigma_{xy}}{N-1}.$$

\subsection{Theorem.} For a uniform sample without replacement, the sample mean has the following properties:
\begin{enumerate}
\item[(1)] The expectation is 
$$\E(\bar{X}) = \mu_{x}.$$
\item[(2)] The variance is 
$$\Var(\bar{X}) = \frac{1}{n}\left(\frac{N-n}{N-1}\right)\sigma^{2}$$
where $(N-n)/(N-1)$ is the finite population correction factor.
\end{enumerate}

\subsection{Proof.} (Continued.) The expectation is the same as before since the sample points are exchangeable. We have that the variance is 
\begin{align*}
    \Var(\bar{X}) &= \frac{1}{n^{2}}\Cov\left(\sum_{i=1}^{n}X_{i}, \sum_{j=1}^{n}X_{j}\right) \\
                  &= \frac{1}{n}\sigma^{2} + \frac{n-1}{n}\Cov(X_{i}, X_{j}) \\
                  &= \frac{1}{n}\sigma^{2} - \frac{n-1}{n}\frac{\sigma^{2}}{N-1} \\
                  &= \frac{1}{n}\left(1 - \frac{n-1}{N-1}\right)\sigma^{2} \\
                  &= \frac{1}{n}\left(\frac{N-n}{N-1}\right)\sigma^{2}.
\end{align*}

\newpage \newsection{Estimators for Finite Population Samples.}

\subsection{Introduction.} Estimators for Finite Population Samples.

\subsection{Definition.} An estimator $\hat{\theta}$ is a function of the sample points $Z_{1}, Z_{2}, \ldots, Z_{n}$ that estimates a population parameter $\theta$.

\subsection{Definition.} The bias of an estimator $\hat{\theta}$ for $\theta$ is 
$$\text{Bias}(\hat{\theta}) = \E(\hat{\theta}) - \theta.$$
$\hat{\theta}$ is unbiased for $\theta$ if
$$\E(\hat{\theta}) = \theta.$$

\subsection{Definition.} An estimator $\hat{\theta}$ for $\theta$ is consistent if for all $\delta > 0$, 
$$P(|\hat{\theta} - \theta| > \delta) \to 0 \text{ as } n \to \infty.$$

\subsection{Definition.} For an estimator $\hat{\theta}$ of a parameter $\theta$, the mean squared error is 
\begin{align*}
    \MSE(\hat{\theta}) &= \E\left((\hat{\theta} - \theta)^{2}\right) \\
                       &= \Var(\hat{\theta}) + \left(\E(\hat{\theta}) - \theta\right)^{2} \\
                       &= \Var(\hat{\theta}) + \text{Bias}(\hat{\theta})^{2}.
\end{align*}

\subsection{Definition.} The sample mean $\bar{X}$ is an estimator of the population mean $\mu_{x}$ defined as 
$$\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_{i}.$$

\subsection{Theorem.} The sample mean $\bar{X}$ is an unbiased estimator of the population mean $\mu_{x}$.

\subsection{Proof.} (Continued.) Consider the expectation
\begin{align*}
    \E(\bar{X}) &= \E\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right) \\
                &= \frac{1}{n}\sum_{i=1}^{n}\E(X_{i}) \\
                &= \mu_{x}.
\end{align*}

\subsection{Example.} $\bar{X}^{2}$ is not an unbiased estimator of $\mu_{x}^{2}$. Observe that 
\begin{align*}
             \E(\bar{X}^{2}) &= \Var(\bar{X}) + \mu_{x}^{2} \\
                             &= \frac{1}{n}\sigma^{2} + \mu_{x}^{2} \\
    \text{Bias}(\bar{X}^{2}) &= \frac{1}{n}\sigma^{2}
\end{align*}
in the case of sampling with replacement and 
$$\text{Bias}(\bar{X}^{2}) = \frac{1}{n}\frac{N-n}{N-1}\sigma^{2}$$
in the case of sampling without replacement.

\subsection{Definition.} The mean squared deviation $\hat{\sigma}^{2}$ is an estimator for the population variance $\sigma^{2}$ defined as 
$$\hat{\sigma}^{2} = \frac{1}{n}\sum_{i=1}^{n}(X_{i} - \bar{X})^{2}.$$

\subsection{Definition.} The sample variance $s^{2}$ is an estimator for the population variance $\sigma^{2}$ defined as 
$$s^{2} = \frac{n}{n-1}\hat{\sigma}^{2}.$$

\subsection{Remark.} For $n$ large, 
$$s^{2} \approx \hat{\sigma}^{2}.$$

\subsection{Theorem.} Suppose that we sample uniformly with replacement. The sample variance $s^{2}$ is an unbiased estimator of the population variance $\sigma^{2}$.

\subsection{Proof.} (Continued.) Consider the expectation 
\begin{align*}
    \E(\hat{\sigma}^{2}) &= \E\left(\frac{1}{n}\sum_{i=1}^{n}(X_{i} - \bar{X})^{2}\right) \\
                         &= \frac{1}{n}\sum_{i=1}^{n}\E(X_{i}^{2}) - \bar{X}^{2} \\
                         &= \Var{X} + \E(X)^{2} - \Var(\bar{X}) - \E(X)^{2} \\
                         &= \Var(X) - \Var(\bar{X}) \\
                         &= \frac{n-1}{n}\sigma^{2}.
\end{align*}
We can correct this bias to obtain the unbiased estimator $s^{2}$ such that 
\begin{align*}
    \E(s^{2}) &= \E\left(\frac{n}{n-1}\hat{\sigma}^{2}\right) \\
              &= \sigma^{2}.
\end{align*}

\subsection{Theorem.} Suppose that we sample uniformly without replacement. The unbiased estimator $\hat{\theta}$ for $\sigma^{2}$ is 
\begin{align*}
    \hat{\theta} &= \frac{n}{n-1}\frac{N-1}{N}\hat{\sigma}^{2} \\
                 &= \frac{N-1}{N}s^{2}.
\end{align*}

\subsection{Proof.} (Continued.) Consider the expectation 
\begin{align*}
    \E(\hat{\sigma^{2}}) &= \frac{1}{n}\sum_{i=1}^{n}\E(X_{i}^{2}) - \E(\bar{X}^{2}) \\
                         &= \Var(X) + \E(X)^{2} - \Var(\bar{X}) - \E(X)^{2} \\
                         &= \Var(X) - \Var(\bar{X}) \\
                         &= \sigma^{2} - \frac{1}{n}\frac{N-n}{N-1}\sigma^{2} \\
                         &= 1 - \frac{1}{n}\frac{N-n}{N-1}\sigma^{2}.
\end{align*}
We then have that 
\begin{align*}
    \E(s^{2}) &= \E\left(\frac{n}{n-1}\hat{\sigma}^{2}\right) \\
              &= \frac{n}{n-1}\left(1 - \frac{1}{n}\frac{N-n}{N-1}\sigma^{2}\right) \\
              &= \frac{n}{n-1}\left(\frac{N(n-1)}{n(N-1)}\right)\sigma^{2} \\
              &= \frac{N}{N-1}\sigma^{2}.
\end{align*}
Hence, the unbiased estimator $\hat{\theta}$ for $\sigma^{2}$ is
$$\hat{\theta} = \frac{n}{n-1}\frac{N}{N-1}\hat{\sigma}^{2}.$$

\subsection{Theorem.} The sample variance $s^{2}$ is a consistent estimator of the population variance $\sigma^{2}$.

\subsection{Example.} $s$ is not an unbiased estimator of $\sigma$. Observe that $f: t \to \sqrt{t}$ is a strictly concave function. Therefore, by Jensen's Inequality, we have that
\begin{align*}
    \E(\sqrt{s^{2}}) &\leq \sqrt{\E(s^{2})} \\
               \E(s) &\leq \sigma
\end{align*}
and the inequality is strict if the sample points $X$ are not degenerate.

\newpage \newsection{Dichotomous Populations.}

\subsection{Introduction.} Dichotomous Populations.

\subsection{Definition.} A population is dichotomous if at least one measurement is binary, that is 
$$x_{k} \in \{0, 1\}$$
for all $k = 1, 2, \ldots, N$ for some measurement $x$.

\subsection{Remark.} Dichotomous populations have a number of nice properties. These properties arise from the Bernoulli nature of the sample points.

\subsection{Definition.} The population proportion is
$$p = \frac{1}{N}\sum_{k=1}^{N}x_{k}.$$
Note that $p$ is also the population mean $\mu$.

\subsection{Theorem.} The population variance of a dichotomous population is bounded by 
$$0 \leq \sigma^{2} \leq \frac{1}{4}.$$

\subsection{Proof.} (Continued.) Since our sample points are Bernoulli, we have that 
$$\Var(X) = p(1-p),$$
which is maximized for $p = 1/2$.

\subsection{Definition.} The sample proportion $\hat{p}$ is an estimator of the population proportion $p$ defined as
$$\hat{p} = \frac{1}{n}\sum_{i=1}^{n}X_{i}.$$
Note that $\hat{p}$ is also the sample mean $\bar{X}$.

\subsection{Theorem.} The sample proportion $\hat{p}$ is an unbiased estimator of the population proportion $p$.

\subsection{Proof.} (Continued.) The sample mean is an unbiased estimator of the population mean, so the sample proportion is an unbiased estimator of the population proportion.

\subsection{Example.} The estimator 
$$\frac{1}{n-1}\hat{p}(1-\hat{p})$$
is unbiased for $\sigma_{\hat{p}}^{2}$.

\subsection{Proof.} (Continued.) Recall that 
$$\sigma_{\hat{p}}^{2} = \frac{1}{n}\sigma^{2}.$$
We have that 
\begin{align*}
    E\left(\frac{\hat{p}(1-\hat{p})}{n-1}\right) &= \frac{1}{n-1}\left(\E(\hat{p}) - \E(\hat{p}^{2})\right) \\
                                                 &= \frac{1}{n-1}(p - \sigma_{\hat{p}}^{2} - p^{2}) \\
                                                 &= \frac{1}{n-1}(p(1-p) - \sigma_{\hat{p}}^{2}) \\
                                                 &= \frac{1}{n-1}(n\sigma_{\hat{p}}^{2} - \sigma_{\hat{p}}^{2}) \\
                                                 &= \sigma_{\hat{p}}^{2}.
\end{align*}

\subsection{Theorem.} Suppose we take a uniform sample of size $n$ from a dichotomous population. Then, for any $\epsilon > 0$ and $\delta > 0$, 
$$n \geq \frac{1}{4\epsilon\delta^{2}}$$
such that 
$$P(|\hat{p} - p| > \delta) \leq \epsilon.$$

\subsection{Proof.} (Continued.) Recall that the population variance is bounded by $0 \leq \sigma^{2} \leq 1/4$. We therefore have that the variance of the sample proportion is 
$$\Var(\hat{p}) \leq \frac{1}{4n}.$$
By Chebyshev's Inequality, we have that 
\begin{align*}
    P(|\hat{p} - p| > \delta) &\leq \frac{\Var(\hat{p})}{\delta^{2}} \\
                              &\leq \frac{1}{4n\delta^{2}}.
\end{align*}
We want 
$$\frac{1}{4n\delta^{2}} \leq \epsilon,$$
so we choose 
$$n \geq \frac{1}{4\epsilon\delta^{2}}.$$

\subsection{Proof.} (Continued.) The sample mean is a consistent estimator of the population mean, so the sample proportion is a consistent estimator of the population proportion.

\subsection{Theorem.} For a bivariate population of two dichotomous measurements, if the population covariance 
$$\sigma_{xy} = 0,$$
then the measurements $X$ and $Y$ are independent.

\subsection{Proof.} (Continued.) Since $X$ and $Y$ are Bernoulli, if they are of covariance zero, then they are independent.

\newpage \newsection{Confidence Intervals.}

\subsection{Introduction.} Confidence Intervals.

\subsection{Theorem.} (Central Limit Theorem.) Let $U_{1}, U_{2}, \ldots, U_{n}$ be iid rvs with $\E(U_{i}) = \mu$ and $\Var(U_{i}) = \sigma^{2}$. For $$\bar{U} = \frac{1}{n}\sum_{i=1}^{n}U_{i},$$
we have that for all $t \in \mathbb{R}$,
$$P\left(\left|\frac{\bar{U} - \mu}{\sigma/\sqrt{n}}\right| \leq t\right) \rightarrow \Phi(t) \text{ as } n \rightarrow \infty.$$

\subsection{Definition.} An $\alpha$-critical value $z_{\alpha}$ for an rv $Z$ is such that 
$$P(Z > z_{\alpha}) = \alpha.$$
That is, $\alpha$ is the upper-tail probability of $Z$.

\subsection{Example.} For $\bar{X}$ approximately normal, we have that 
$$P\left(-z_{\alpha} \leq \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \leq z_{\alpha}\right) \approx 1 - 2\alpha,$$
so
$$P\left(\bar{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right) \approx 1 - \alpha$$
where the (random) interval is the $1-\alpha$ confidence interval for $\mu$.

\subsection{Note.} The population standard deviation $\sigma$ may be unknown, but we may substitute the sample standard deviation $s$ in its place.

\subsection{Example.} By Chebyshev's Inequality, we have that for a sample of size $n$, 
$$P(|s_{n}^{2} - \sigma^{2}| \geq \delta) \leq \frac{\Var(s_{n}^{2})}{\delta^{2}} \rightarrow 0 \text{ as } n \rightarrow \infty,$$
so 
$$\frac{\bar{X} - \mu}{s/\sqrt{n}} \approx \text{N}(0, 1)$$
for $n$ large.

\subsection{Example.} If we sample without replacement and $n << N$, then 
$$\frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}\sqrt{\frac{N-n}{N-1}}} \approx \text{N}(0, 1).$$

\subsection{Theorem.} For the sample total 
$$T_{n} = \sum_{i=1}^{n}X_{i},$$
the CLT says that 
$$P\left(\frac{T_{n} - n\mu}{\sigma\sqrt{n}} \leq t\right) \rightarrow \Phi(t) \text{ as } n \rightarrow \infty.$$

\subsection{Note.} The sample mean and the sample total are related in that 
$$\frac{n}{n}\frac{\bar{X}_{n}-\mu}{\sigma/\sqrt{n}} = \frac{T_{n}-n\mu}{\sigma\sqrt{n}}.$$

\newpage \newsection{Delta Methods.}

\subsection{Introduction.} Delta Methods.

\subsection{Theorem.} (Mean Value Theorem.) Suppose that $g: \mathbb{R} \to \mathbb{R}$ is differentiable on $(a, b)$ and that $a < x < y < b$. Then there exists a $\xi \in [x, y]$ such that 
$$g'(\xi) = \frac{g(y) - g(x)}{y - x}.$$

\subsection{Theorem.} (Taylor's Theorem with Remainder.) 
Let $g: \mathbb{R} \to \mathbb{R}$ be $n$ times differentiable on $(a, b)$ and let $a < x < y < b$. Then there exists a $\xi \in [x, y]$ such that 
$$g(y) = \sum_{k=0}^{n}\frac{g^{(k)}(x)}{k!}(y-x)^{k} + R_{n}(y)$$
where
$$R_{n}(y) = \frac{g^{(n+1)}(\xi)}{(n+1)!}(y-x)^{n+1}.$$
We may bound this remainder by 
$$|R_{n}(y)| \leq \frac{M}{(n+1)!}|y-x|^{n+1}$$
where $M = \max|g^{(n+1)}(\xi)|$ on the interval $(x, y)$.

\subsection{Theorem.} For $g: \mathbb{R}^{n} \to \mathbb{R}$ twice-differentiable on a closed ball $B$ containing $x$ and $y$, we have that the first-order Taylor polynomial with remainder is 
$$g(y) = g(x) + \nabla g(x)^{T}(y-x) + \frac{1}{2}(y-x)^{T}H(\xi)(y-x)$$
where 
$$\nabla g(x) = \begin{pmatrix} \frac{\partial g}{\partial x_{1}}(\xi) & \cdots & \frac{\partial g}{\partial x_{n}}(\xi) \end{pmatrix}^{T}$$
for some $\xi \in \mathbb{R}^{n}$ between $x$ and $y$ where is the gradient of $g$ at $x$ and 
$$H(\xi) = \begin{pmatrix} \frac{\partial^{2} g}{\partial x_{1}^{2}}(\xi) & \cdots & \frac{\partial^{2} g}{\partial x_{1}\partial x_{n}}(\xi) \\ \vdots & \ddots & \vdots \\ \frac{\partial^{2} g}{\partial x_{n}\partial x_{1}}(\xi) & \cdots & \frac{\partial^{2} g}{\partial x_{n}^{2}}(\xi) \end{pmatrix}$$
is the Hessian of $g$ at $\xi$.

\subsection{Example.} For $g: \mathbb{R}^{2} \to \mathbb{R}$, the second order taylor polynomial is 
$$g(y) \approx g(x) + \nabla g(x)^{T}(y-x) + \frac{1}{2}(y-x)^{T}H(x)(y-x).$$
Written in polynomial form, this is 
\begin{align*}
    g(y_{1}, y_{2}) \approx g(x_{1}, x_{2}) &+ \frac{\partial g}{\partial x_{1}}(y_{1}-x_{1}) + \frac{\partial g}{\partial x_{2}}(y_{2}-x_{2}) \\
                                            &+ \frac{1}{2}\left(\frac{\partial^{2} g}{\partial x_{1}^{2}}(y_{1}-x_{1})^{2} + 2\frac{\partial^{2} g}{\partial x_{1}\partial x_{2}}(y_{1}-x_{1})(y_{2}-x_{2}) + \frac{\partial^{2} g}{\partial x_{2}^{2}}(y_{2}-x_{2})^{2}\right)
\end{align*}
where the partial derivatives are evaluated at $(x_{1}, x_{2})$.

\subsection{Definition.} For a random vector $X = (X_{1}, X_{2})$ and a function $g: \mathbb{R}^{2} \to \mathbb{R}$, the delta method is a method to approximate the distribution of $g(X)$ with a Taylor polynomial about $\mu_{x}$.

\subsection{Example.} For $h: \mathbb{R}^{2} \to \mathbb{R}$, the second-order taylor polynomial about $(\mu_{x}, \mu_{y})$ is
\begin{align*}
    h(x, y) \approx h(\mu_{x}, \mu_{y}) &+ \frac{\partial h}{\partial x}(x - \mu_{x}) + \frac{\partial h}{\partial y}(y - \mu_{y}) \\
                                        &+ \frac{1}{2}\left(\frac{\partial^{2} h}{\partial x^{2}}(x - \mu_{x})^{2} + 2\frac{\partial^{2} h}{\partial x \partial y}(x - \mu_{x})(y - \mu_{y}) + \frac{\partial^{2} h}{\partial y^{2}}(y - \mu_{y})^{2}\right)
\end{align*}
where the partial derivatives are evaluated at $(\mu_{x}, \mu_{y})$. Therefore, the expected value of $h(X, Y)$ (under certain conditions) is 
$$\E(h(X, Y)) \approx h(\mu_{x}, \mu_{y}) + \frac{1}{2}\frac{\partial^{2} h}{\partial x^{2}}\sigma_{X}^{2} + \frac{\partial^{2} h}{\partial x \partial y}\sigma_{XY} + \frac{1}{2}\frac{\partial^{2} h}{\partial y^{2}}\sigma_{Y}^{2}$$
because the first-order terms vanish when 
$$\E(X - \mu_{x}) = 0.$$

\subsection{Example.} For $h: \mathbb{R}^{2} \to \mathbb{R}$, the first-order taylor polynomial about $(\mu_{X}, \mu_{Y})$ is
$$h(X, Y) \approx h(\mu_{X}, \mu_{Y}) + \frac{\partial h}{\partial x}(X - \mu_{X}) + \frac{\partial h}{\partial y}(Y - \mu_{Y})$$
where the partial derivatives are evaluated at $(\mu_{X}, \mu_{Y})$. Therefore, the variance of $h(X, Y)$ (under certain conditions) is 
$$\Var(h(X, Y)) \approx \left(\frac{\partial h}{\partial x}\right)^{2}\Var(X) + \left(\frac{\partial h}{\partial y}\right)^{2}\Var(Y) + 2\frac{\partial h}{\partial x}\frac{\partial h}{\partial y}\Cov(X, Y).$$
To approximate the variance, the second-order terms become small quickly, so a first-order approximation is appropriate.

\newpage \newsection{Sample Ratio.}

\subsection{Introduction.} Sample Ratio.

\subsection{Definition.} For a bivariate population and a sample of size $n$, the sample ratio is 
$$\bar{R} = \frac{\bar{Y}}{\bar{X}}.$$

\subsection{Example.} We then have that 
$$g(x, y) = \frac{y}{x}$$
with partial derivatives 
$$\frac{\partial g}{\partial x} = -\frac{y}{x^{2}}, \quad \frac{\partial g}{\partial y} = \frac{1}{x}$$
$$\frac{\partial^{2} g}{\partial x^{2}} = \frac{2y}{x^{3}}, \quad \frac{\partial^{2} g}{\partial x \partial y} = -\frac{1}{x^{2}}, \quad \frac{\partial^{2} g}{\partial y^{2}} = 0.$$
Therefore, 
\begin{align*}
    \bar{R} \approx \frac{\mu_{y}}{\mu_{x}} &- \frac{\mu_{y}}{\mu_{x}^{2}}(\bar{X} - \mu_{x}) + \frac{1}{\mu_{x}}(\bar{Y} - \mu_{y}) \\
                                            &+ \frac{\mu_{y}}{\mu_{x}^{3}}(\bar{X} - \mu_{x})^{2} - \frac{1}{\mu_{x}^{2}}(\bar{X} - \mu_{x})(\bar{Y} - \mu_{y}).
\end{align*}

\subsection{Example.} (Continued.) The expected value of $\bar{R}$ is 
\begin{align*}
    \E(\bar{R}) &\approx \frac{\mu_{y}}{\mu_{x}} + \frac{1}{\mu_{x}^{2}}\left(\Var(\bar{X})\frac{\mu_{y}}{\mu_{x}} - \Cov(\bar{X}, \bar{Y})\right) \\
                &= r + \frac{1}{\mu_{x}^{2}}(\Var(\bar{X})r - \Cov(\bar{X}, \bar{Y}))
\end{align*}
where 
$$r = \frac{\mu_{y}}{\mu_{x}}$$
In the case of sampling with replacement, we have that
$$\E(\bar{R}) = \frac{\mu_{y}}{\mu_{x}} + \frac{1}{\mu_{x}^{2}}\left(\frac{\mu_{y}}{\mu_{x}}\frac{\sigma_{x}^{2}}{n} - \frac{\sigma_{xy}}{n}\right).$$
In the case of sampling without replacement, we have that 
$$\E(\bar{R}) = \frac{\mu_{y}}{\mu_{x}} + \frac{1}{\mu_{x}^{2}}\left(\frac{\mu_{y}}{\mu_{x}}\frac{\sigma_{x}^{2}}{n}\frac{N - n}{N - 1} - \frac{\sigma_{xy}}{n}\frac{N - n}{N - 1}\right).$$

\subsection{Example.} (Continued.) The variance of $\bar{R}$ is 
$$\Var(\bar{R}) = \frac{\mu_{y}^{2}}{\mu_{x}^{4}}\sigma_{\bar{x}}^{2} + \frac{1}{\mu_{x}^{2}}\sigma_{\bar{y}}^{2} - \frac{2\mu_{y}}{\mu_{x}^{3}}\sigma_{\bar{x}\bar{y}}.$$
In the case of sampling with replacement, we have that 
\begin{align*}
    \Var(\bar{R}) &= \frac{\mu_{y}^{2}}{\mu_{x}^{4}}\frac{\sigma_{x}^{2}}{n} + \frac{1}{\mu_{x}^{2}}\frac{\sigma_{y}^{2}}{n} - \frac{2\mu_{y}}{\mu_{x}^{3}}\frac{\sigma_{xy}}{n} \\
            &= \frac{1}{\mu_{x}^{2}}\frac{1}{n}\left(\frac{\mu_{y}^{2}}{\mu_{x}^{2}}\sigma_{x}^{2} + \sigma_{y}^{2} 
               - \frac{2\mu_{y}}{\mu_{x}}\sigma_{xy}\right).
\end{align*}
and in the case of sampling without replacement, we have that 
\begin{align*}
    \Var(\bar{R}) &= \frac{\mu_{y}^{2}}{\mu_{x}^{4}}\frac{\sigma_{x}^{2}}{n}\frac{N - n}{N - 1} + \frac{1}{\mu_{x}^{2}}\frac{\sigma_{y}^{2}}{n}\frac{N - n}{N - 1} - \frac{2\mu_{y}}{\mu_{x}^{3}}\frac{\sigma_{xy}}{n}\frac{N - n}{N - 1} \\
            &= \frac{1}{\mu_{x}^{2}}\frac{1}{n}\frac{N-n}{N-1}\left(\frac{\mu_{y}^{2}}{\mu_{x}^{2}}\sigma_{x}^{2} + \sigma_{y}^{2} - \frac{2\mu_{y}}{\mu_{x}}\sigma_{xy}\right).
\end{align*}
where $n, N$ are the sample size and population size, respectively.

\subsection{Theorem.} We have the following propositions from Rice, Chapter (7), Section (7.4). Consider the case of sampling without replacement. Taking 
$$r = \frac{\mu_{x}}{\mu_{y}},$$
we arrive at Theorem B, that the approximate expectation of $R = \bar{Y}/\bar{X}$ is 
$$\E(R) \approx r + \frac{1}{n}\left(1 - \frac{n-1}{N-1}\right)\frac{1}{\mu_{x}^{2}}
(r\sigma_{x}^{2} - \rho\sigma_{x}\sigma_{y})$$
where $\rho$ is the correlation 
$$\rho = \frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}$$
as well as Corollary B, that the approximate bias of the ratio estimate $\bar{Y}_{R} = \mu_{x}R$ of $\mu_{y}$ is 
$$\E(\bar{Y}_{R}) - \mu_{y} \approx \frac{1}{n}\left(1 - \frac{n-1}{N-1}\right)\frac{1}{\mu_{x}}\sigma_{x}^{2}.$$

\subsection{Theorem.} We have the following propositions from Rice, Chapter (7), Section (7.4). Consider the case of sampling without replacement. Taking 
$$r = \frac{\mu_{x}}{\mu_{y}},$$
we arrive at Theorem A, that the approximate variance of $R = \bar{Y}/\bar{X}$ is 
$$\Var(R) \approx \frac{1}{n}\left(1 - \frac{n-1}{N-1}\right)\frac{1}{\mu_{x}^{2}}
                  (r^{2}\sigma_{x}^{2} + \sigma_{y}^{2} - 2r\sigma_{xy}),$$
and Corollary A, that the the estimated variance of the ratio estimate $\bar{Y}_{R} = \mu_{x}R$ of $\mu_{y}$ is 
$$\Var(\bar{Y}_{R}) \approx \frac{1}{n}\left(1 - \frac{n-1}{N-1}\right)\left(r^{2}\sigma_{y}^{2} 
                  + \sigma_{x}^{2} - 2r\sigma_{xy}\right),$$
and Corollary C, that the variance of $\bar{Y}_{R}$ can be estimated by 
$$s^{2}_{\bar{Y}_{R}} \approx \frac{1}{n}\left(1 - \frac{n-1}{N-1}\right)(R^{2}s_{x}^{2} + s_{y}^{2} - 2Rs_{xy}).$$

\subsection{Theorem.} (Rice, Chapter (7), Exercise (50).) Hartley and Ross (1954) derived the following exact bound on the relative size of the bias and 
standard error of a ratio estimate:
$$\frac{|\E(R) - r|}{\sigma_{R}} \leq \frac{\sigma_{\bar{X}}}{\mu_{x}}.$$

\subsection{Proof.} (Continued.) Consider the relation
$$\Cov(R, \bar{X}) = \E(R\bar{X}) - \E(R)\E(\bar{X}).$$
We have that 
\begin{align*}
    E(R\bar{X}) - E(R)E(\bar{X}) &= E(\bar{Y}) - E(R)E(\bar{X}) \\
                                 &= \mu_{y} - \mu_{x}E(R)
\end{align*}
so by the Cauchy-Schwarz inequality, we have that 
\begin{align*}
    |\mu_{y} - \mu_{x}E(R)| &\leq \sigma_{\bar{x}}\sigma_{R} \\
    |\mu_{x}E(R) - \mu_{y}| &\leq \sigma_{\bar{x}}\sigma_{R} \\
    |E(R) - r| &\leq \frac{\sigma_{\bar{x}}\sigma_{R}}{\mu_{x}} \\
    \frac{|E(R) - r|}{\sigma_{R}} &\leq \frac{\sigma_{\bar{x}}}{\mu_{x}}.
\end{align*}

\textbf{Remark.} For the ratio estimate of the population total 
$$T_{R} = \tau_{x}R,$$
the squared standard error for $T_{R}$ is 
$$s_{T_{R}}^{2} = N^{2}\frac{1}{n}\left(\frac{N-n}{N-1}\right)(R^{2}s_{x}^{2} + s_{y}^{2} - 2Rs_{xy}).$$
Compare that to the standard error for the direct estimate $T$ in part (c), which is 
$$s_{T}^{2} = N^{2}\frac{1}{n}\left(\frac{N-n}{N-1}\right)s_{y}^{2}.$$
If $R$ is small or if $s_{x}$ is small, then 
\begin{align*}
    R^{2}s_{x}^{2} + s_{y}^{2} - 2Rs_{xy} &< s_{y}^{2} \\
                            s_{T_{R}}^{2} &< s_{T}^{2}.
\end{align*}
The same argument holds for the variance of the ratio estimate 
$$\bar{Y}_{R} = \mu_{x}R.$$
This is an example of a biased estimator possessing a smaller variance than the unbiased estimator.

\newpage \newsection{Real Analysis.}

\subsection{Introduction.} Real Analysis.

\subsection{Definition.} A field is a set $F$ equipped with two operations: addition and multiplication. The field axioms are as follows.
\begin{enumerate}
\item[(A)] Addition.
    \begin{enumerate}
    \item[(A1)] If $x,y \in F$, then $x+y \in F$. (Closure.)
    \item[(A2)] If $x,y \in F$, then $x+y = y+x$. (Commutativity.)
    \item[(A3)] If $x,y,z \in F$, then $(x+y)+z = x+(y+z)$. (Associativity.)
    \item[(A4)] There exists an element $0 \in F$ such that $0 + x = x$ for all $x \in F$. (Identity.)
    \item[(A5)] To every $x \in F$ there corresponds an element $-x \in F$ such that $x + (-x) = 0$. (Inverse.)
    \end{enumerate}
\item[(M)] Multiplication.
    \begin{enumerate}
    \item[(M1)] If $x,y \in F$, then $xy \in F$. (Closure.)
    \item[(M2)] If $x,y \in F$, then $xy = yx$. (Commutativity.)
    \item[(M3)] If $x,y,z \in F$, then $(xy)z = x(yz)$. (Associativity.)
    \item[(M4)] There exists an element $1 \in F, 1 \neq 0$, such that $1x = x$ for all $x \in F$. (Identity.)
    \item[(M5)] If $x \in F, \neq 0$, then there corresponds an element $1/x \in F$ such that $x(1/x) = 1$. (Inverse.)
    \end{enumerate}
\item[(D)] Distribution.
    \begin{enumerate}
    \item[(D1)] If $x,y,z \in F$, then $x(y+z) = xy + xz$. (Left distribution.)
    \end{enumerate}
\end{enumerate}

\subsection{Definition.} An ordered set is a set $S$ equipped with a relation $<$ such that for all $x,y,z \in S$,
\begin{enumerate}
\item[(1)] If $x,y \in S$, then one and only one of 
$$x < y, \quad x = y, \quad y < x$$
is true. (Trichotomy.)
\item[(2)] If $x,y,z \in S$ and $x < y$ and $y < z$, then $x < z$. (Transitivity.)
\end{enumerate}

\subsection{Definition.} An ordered field is a field $F$ equipped with an order relation $<$ such that for all $x,y,z \in F$, 
\begin{enumerate}
\item[(1)] If $x,y,z \in F$ and $y < z$, then $x + y < x + z$.
\item[(2)] If $x,y \in F$ and $x,y > 0$, then $xy > 0$.'
\end{enumerate}

\subsection{Remark.} From these axioms, we may derive the familiar properties of $\mathbb{Q}, \mathbb{R}, \mathbb{C}$.

\subsection{Definition.} A subset $D$ of an ordered field $F$ is said to be bounded above if there exists an element $M \in F$ such that 
$$x \leq M, \quad \forall x \in D.$$
The element $M$ is called an upper bound of $D$. $M$ is a least upper bound of $D$ if
\begin{enumerate}
\item[(1)] $\forall x \in D, M \leq x$.
\item[(2)] $\forall m < M, \exists x \in D \text{ s.t. } m < x$.
\end{enumerate}

\subsection{Definition.} The least upper bound property states that every nonempty subset $D$ of $F$ that is bounded above has a least upper bound
$$\sup D.$$

\subsection{Theorem.} There exists an ordered field $\mathbb{R}$ with the least upper bound property. Moreover, $\mathbb{Q} \subset \mathbb{R}$.

\subsection{Definition.} A metric space is a set $X$ equipped with a metric $d: X \times X \to \mathbb{R}$ such that for all $x,y,z \in X$,
\begin{enumerate}
\item[(1)] $d(x,y) \geq 0$. (Non-negativity.)
\item[(2)] $d(x,y) = 0$ if and only if $x = y$. (Positive definiteness.)
\item[(3)] $d(x,y) = d(y,x)$. (Symmetry.)
\item[(4)] $d(x,y) \leq d(x,z) + d(z,y)$. (Triangle inequality.)
\end{enumerate}
Unless otherwise specified, we assume that the standard metric is 
$$d(x,y) = |x-y|.$$

\subsection{Definition.} A sequence $\{x_{n}\}$ in $\mathbb{R}$ is the indexed output of a map 
$$\phi: \mathbb{N} \to \mathbb{R}$$
and we denote the sequence as 
$$\{a_{n}: n \in \mathbb{N}\}.$$
or simply as $\{a_{n}\}$ or even more simply as $a_{n}$.

\subsection{Definition.} A sequence is Cauchy if 
$$\forall \epsilon > 0, \exists N_{\epsilon} \in \mathbb{N} \text{ s.t. } d(a_{n}, a_{m}) < \epsilon, \forall n,m \geq N_{\epsilon}.$$

\subsection{Definition.} A sequence $\{a_{n}\}$ in $\mathbb{R}$ is convergent if 
$$\exists L \in \mathbb{R} \text{ s.t. } \forall \epsilon > 0, \exists N_{\epsilon} \in \mathbb{N} \text{ s.t. } d(a_{n} - L) < \epsilon, \forall n \geq N_{\epsilon}.$$
$L$ is said to be the limit of the sequence $\{a_{n}\}$.

\subsection{Lemma.} A sequence is Cauchy if it is convergent.

\subsection{Proof.} (Continued.) Suppose that $\epsilon > 0$. Since 
$$a_{n} \rightarrow L,$$
there exists an $N_{\epsilon/2} \in \mathbb{N}$ such that for all $n > N_{\epsilon/2}$,
\begin{align*}
        d(a_{n}, L) &< \frac{\epsilon}{2} \\
        d(a_{m}, L) &< \frac{\epsilon}{2} \\
    d(a_{n}, a_{m}) &< \epsilon
\end{align*}
for all $m, n > N_{\epsilon/2}$ by the triangle inequality. Hence, a sequence if convergent if it is Cauchy.

\subsection{Definition.} A set $D$ is complete if every Cauchy sequence in $D$ is convergent to a limit $L \in D$.

\subsection{Theorem.} $\mathbb{R}$ is complete.

\subsection{Proof.} (Continued.) Suppose that $a_{n}$ is a Cauchy sequence in $\mathbb{R}$ that is not bounded. Then, for all $\epsilon > 0$, there exists an $N_{\epsilon} \in \mathbb{N}$ such that
$$d(a_{n}, a_{m}) < \epsilon, \forall n,m \geq N_{\epsilon}.$$
But $a_{n}$ is not bounded, so for any $\epsilon > 0$ and $m \in \mathbb{N}$, there exists an $n \geq m$ such that 
$$d(a_{n}, a_{m}) \geq \epsilon$$
because $a_{m} \pm \epsilon$ would otherwise be a bound for $a_{n}$. Hence, we have a contradiction, so $\mathbb{R}$ is complete.

\subsection{Lemma.} $x$ such that $x^{2} = 2$ is irrational.

\subsection{Proof.} (Continued.) Suppose that $x$ such that $x^{2} = 2$ is rational, i.e., that $x = p/q$ for some $p,q \in \mathbb{Z}$ coprime. Then, 
\begin{align*}
    \frac{p^{2}}{q^{2}} &= 2 \\
                  p^{2} &= 2q^{2}.
\end{align*}
Hence, $p^{2}$ is even, which means that $p$ is even. Let $p = 2k$ for some $k \in \mathbb{Z}$. Then, 
\begin{align*}
    4k^{2} &= 2q^{2} \\
    2k^{2} &= q^{2}.
\end{align*}
Hence, $q^{2}$ is even, which means that $q$ is even. But $p$ and $q$ are coprime, so we have a contradiction. Therefore, $x$ such that $x^{2} = 2$ is irrational.

\subsection{Theorem.} $\mathbb{Q}$ is not complete.

\subsection{Proof.} (Continued.) Recall that a field is complete if every Cauchy sequence in the field converges to a limit in the field We shall construct a Cauchy sequence in $\mathbb{Q}$ that does not converge to a limit in $\mathbb{Q}$. 

\textit{The Sequence.} Consider the function $f(x) = x^{2} - 2$. Recall Newton's method for finding roots of $y = f(x)$, wherein iterates are defined as 
$$x_{n+1} = x_{n} - \frac{f(x_{n})}{f'(x_{n})}.$$
We have that 
\begin{align*}
    x_{n+1} &= x_{n} - \frac{x_{n}^{2} - 2}{2x_{n}} \\
            &= \frac{1}{2}x_{n} + \frac{1}{x_{n}}.
\end{align*}
\textit{Boundedness.} Suppose that $x_{n} = \sqrt{2} + \epsilon$ for some $\epsilon > 0$. We then have that 
\begin{align*}
    x_{n+1} &= \frac{1}{2}(\sqrt{2} + \epsilon) + \frac{1}{\sqrt{2} + \epsilon} \\
            &= \sqrt{2} + \frac{\epsilon^{2}}{2(\sqrt{2} + \epsilon)} \\
    x_{n+1} &> \sqrt{2}.
\end{align*}
For $\epsilon > 0$,  the inequality always holds, i.e., if $x_{n} > \sqrt{2}$, then $x_{n+1} > \sqrt{2}$. 

\textit{Monotonacity.} We also have that 
\begin{align*}
    x_{n+1} - x_{n} &= \sqrt{2} +  \frac{\epsilon^{2}}{2(\sqrt{2} + \epsilon)} - \sqrt{2} - \epsilon \\
    x_{n+1} - x_{n} &\leq 0.
\end{align*}
For $\epsilon > 0$, the inequality always holds, i.e., if $x_{n} > \sqrt{2}$, then $x_{n+1} \leq x_{n}$. In fact, the sequence is strictly decreasing such that $x_{n+1} < x_{n}$.

\textit{Convergence.} The sequence $x_{n}$ is nonempty and bounded below, so it must have a greatest lower bound $L$. Suppose that $x_{n}$ does not converge to $L$. Then, 
$$\exists\epsilon>0 \text{ s.t. } \neg\exists N \in \mathbb{N} \text{ s.t. } |x_{n}-L|<\epsilon, \quad \forall n \geq N.$$
Recall that $L$ is a lower bound, so the only way for this statement to hold is if 
$x_{n} \geq L + \epsilon$ for all $n \geq N$. But then $L + \epsilon$ is a lower bound, which means that $L$ is not the 
greatest lower bound, hence a contradiction. Therefore, the sequence $x_{n}$ converges to $L$. 

Observe that $L$ cannot be strictly less than $\sqrt{2}$ because $\sqrt{2}$ is a lower bound of $x_{n}$. Observe also that $L$ cannot be strictly greater than $\sqrt{2}$ because we define $x_{1} = \sqrt{2} + \epsilon$ for some $\epsilon > 0$ and the sequence is strictly decreasing. Hence, $L = \sqrt{2}$.

\textit{Rationality.} Suppose that $x_{1} = 3/2$. Then, $x_{n} \in \mathbb{Q}$ for all $n \in \mathbb{N}$. We shall prove this fact by induction. 
\begin{enumerate}
\item[(1)] \textit{Base Case.} $x_{1} = 3/2 \in \mathbb{Q}$.
\item[(2)] \textit{Inductive Hypothesis.} Suppose that $x_{n} \in \mathbb{Q}$ for some $n \in \mathbb{N}$, i.e., that $x_{n} = p/q$ for some $p,q \in \mathbb{Z}$ coprime.
\item[(3)] \textit{Inductive Step.} We have that 
\begin{align*}
    x_{n+1} &= \frac{1}{2}x_{n} + \frac{1}{x_{n}} \\
            &= \frac{1}{2}\frac{p}{q} + \frac{q}{p} \in \mathbb{Q}
\end{align*}
because the sums and products of rationals are rational. Therefore, each element $x_{n}, n \in \mathbb{N}$, for 
$x_{1} = 3/2$, is in $\mathbb{Q}$.
\end{enumerate}

\textit{Conclusion.} We have hence constructed a sequence $x_{n}$, for $x_{1} = 3/2$, that is in $\mathbb{Q}$ but converges to a limit $L = \sqrt{2}$ that is not in $\mathbb{Q}$. Therefore, $\mathbb{Q}$ is not complete.

\subsection{Definition.} A series is a sequence $s_{n}$ of partial sums 
$$s_{n} = \sum_{k=1}^{n}a_{k}.$$
for some sequence $\{a_{n}\}$.

\subsection{Lemma.} Suppose that $a_{n} \in \mathbb{R}, a_{n} \geq 0$, i.e., that the series $s_{n}$ is monotone increasing. Then, if $s_{n}$ is bounded above, then $s_{n}$ converges to its least supper bound, i.e., 
$$\lim_{n \to \infty}s_{n} = \sup s_{n}.$$
We often denote this limit as 
$$S = \sup s_{n}.$$

\newpage \newsection{Probability Spaces.}

\subsection{Introduction.} Probability Spaces.

\subsection{Definition.} A $\sigma-$algebra $\mathcal{F}$ is a collection of subsets of $\Omega$ such that 
\begin{enumerate}
\item[(1)] $\emptyset \in \mathcal{F}$.
\item[(2)] If $A \in \mathcal{F}$, then $A^{c} \in \mathcal{F}$. (Closure under complement.)
\item[(3)] If $A_{1}, A_{2}, \ldots \in \mathcal{F}$, then 
$$\bigcup_{i=1}^{\infty}A_{i} \in \mathcal{F}.$$
(Closure under countable union.)
\end{enumerate}

\subsection{Definition.} The $\sigma$-algebra generated by a collection of subsets $\mathcal{A}$ is the smallest $\sigma$-algebra containing $\mathcal{A}$. We denote the $\sigma$-algebra generated by $\mathcal{A}$ as
$$\sigma(\mathcal{A}).$$

\subsection{Example.} The $\sigma$-algebra generated by $A$ is 
$$\sigma(A) = \{\emptyset, A, A^{c}, \Omega\}.$$

\subsection{Lemma.} If $A_{1}, A_{2}, \ldots, A_{n}$ partition $\Omega$, then 
$$\sigma(A_{1}, A_{2}, \ldots, A_{n}) = \bigcup_{i \in S}A_{i}$$
for all $S \subseteq \{1, 2, \ldots, n\}$. That is, 
$$\sigma(A_{1}, A_{2}, \ldots, A_{n}) = 2^{\Omega}.$$

\subsection{Example.} Consider the coin-flip space 
$$\Omega = \{\omega = (\omega_{1}, \omega_{2}, \ldots): \omega_{i} \in \{0, 1\}\},$$
that is, the sample space consisting of countable-length strings of $0$ and $1$. Denote 
\begin{align*}
         A_{x} &= \{\omega \in \Omega: \omega_{1} = x\} \\
        A_{xy} &= \{\omega \in \Omega: \omega_{1} = x, \omega_{2} = y\}
\end{align*}
We claim that the $\sigma$-algebra $\mathcal{F}$ generated by the events 
$$A_{1}, \quad A_{11}, \quad A_{01}$$
has cardinality $|\mathcal{F}| = 2^{4}$. Observe that we can form a partition of $\Omega$ by 
$$P = \{A_{00}, A_{01}, A_{10}, A_{11}\}.$$
Therefore, 
$$\mathcal{F} = 2^{P}$$
with cardinality $|\mathcal{F}| = 2^{4}$.

\subsection{Definition.} A probability measure $P$ is a function $P: \mathcal{F} \to [0, 1]$ such that 
\begin{enumerate}
\item[(1)] $P: \mathcal{F} \to [0, 1]$. (Non-negativity.)
\item[(2)] If $A_{1}, A_{2}, \ldots \in \mathcal{F}$ are disjoint, then 
$$P\left(\bigcup_{i=1}^{\infty}A_{i}\right) = \sum_{i=1}^{\infty}P(A_{i}).$$
(Countable additivity.)
\end{enumerate}

\subsection{Definition.} A probability space is a triple $(\Omega, \mathcal{F}, P)$ where the event space $F$ is a $\sigma$-algebra of subsets of the sample space $\Omega$ and $P: \mathcal{F} \to [0, 1]$ is a probability measure on $\mathcal{F}$.

\subsection{Definition.} A real-valued random variable is a function 
$$X: \Omega \to \mathbb{R}$$
with a $(F, \mathcal{B}(\mathbb{R}))$-measurability requirement. That is, for all $B \in \mathcal{B}(\mathbb{R})$, 
$$X^{-1}(B) \in \mathcal{F}$$
where 
$$X^{-1}(B) = \{\omega \in \Omega: X(\omega) \in B\}$$
is the preimage of $B$ under $X$ and where $\mathcal{B}(\mathbb{R})$ is the Borel $\sigma$-algebra on $\mathbb{R}$, i.e., the $\sigma$-algebra generated by the open intervals of $\mathbb{R}$.

\subsection{Example.} Consider the random variable $X = I_{A}$, that is, indicator function of the event $A$. Then, for all $B \in \mathcal{B}(\mathbb{R})$, 
\begin{enumerate}
\item[(1)] If $0 \not\in B$ and $1 \not\in B$, then $X^{-1}(B) = \emptyset$.
\item[(2)] If $0 \not\in B$ and $1 \in B$, then $X^{-1}(B) = A$.
\item[(3)] If $0 \in B$ and $1 \not\in B$, then $X^{-1}(B) = A^{c}$.
\item[(4)] If $0 \in B$ and $1 \in B$, then $X^{-1}(B) = \Omega$.
\end{enumerate}
Therefore, $X$ is a random variable for 
$$\mathcal{F} = \{\emptyset, A, A^{c}, \Omega\}.$$

\subsection{Example.} Consider the sample space of two die rolls, that is 
$$\Omega = \{(i, j): i, j \in \{1, 2, 3, 4, 5, 6\}\}.$$
Let $X(\omega) = \omega_{1} + \omega_{2}$ be the sum of the two die rolls. Consider $B$ such that $B \cap \Omega = 3$ with preimage 
$$X^{-1}(B) = \{(1, 2), (2, 1)\}.$$
So $\mathcal{F}$ must include $\{(1, 2), (2, 1)\}$. But if $A$ includes $(1, 2)$, then it must also include $(2, 1)$. Therefore, 
$$\sigma(X^{-1}(\mathcal{B}(\mathbb{R}))) \neq 2^{\Omega}.$$
In fact, for any $B \in \mathcal{B}(\mathbb{R})$,
$$X^{-1}(B) = X^{-1}(B \cap \text{image}(X)) .$$
In particular, $\mathcal{F}$ is the $\sigma$-algebra generated by 
$$A = \{X^{-1}(\{2\}), X^{-1}(\{3\}), \ldots, X^{-1}(\{12\})\},$$
i.e., 
$$\mathcal{F} = 2^{A}.$$

\subsection{Example.} (Continued.) Recall that 
$$\mathcal{F} = 2^{A}.$$
Consider the random variable $$W_{1} = \omega_{1}$$
for $\omega \in \Omega$. But the preimage 
$$W_{1}^{-1}(1) = \{(1, 1), (1, 2), \ldots, (1, 6)\},$$
is not in $\mathcal{F}$, so $W_{1}$ is not a random variable defined on the specified probability space.

\subsection{Definition.} If $X: \Omega \to \mathbb{R}$ is a random variable, then the $\sigma$-algebra generated by $X$ is 
$$\sigma(X) = \sigma\left(\{X^{-1}(B): B \in \mathcal{B}(\mathbb{R})\}\right).$$

\newpage \newsection{Convergence of Random Variables.}

\subsection{Introduction.} Convergence of Random Variables.

\subsection{Definition.} Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let 
$$\{X_{n}: n \in \mathbb{N}\}$$
be a sequence of rvs on $\Omega$ and let $X$ also be an rv on $\Omega$. Then, $X_{n}$ converges pointwise everywhere to $X$ if for all $\omega \in \Omega$,
$$X_{n}(\omega) \to X(\omega).$$
That is, $X_{n}$ converges pointwise everywhere to $X$ if 
$$\forall \omega \in \Omega, \forall \epsilon > 0, \exists N_{\omega,\epsilon} \in \mathbb{N} \text{ s.t. } |X_{n}(\omega) - X(\omega)| < \epsilon, \forall n > N_{\omega,\epsilon}.$$

\subsection{Definition.} Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let 
$$\{X_{n}: n \in \mathbb{N}\}$$
be a sequence of rvs on $\Omega$ and let $X$ also be an rv on $\Omega$. Then, $X_{n}$ converges pointwise with probability one to $X$ if there exists a set $A$ with $P(A) = 0$ such that 
$$\forall \omega \in A^{c}, X_{n}(\omega) \to X(\omega).$$

\subsection{Definition.} Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let
$$\{X_{n}: n \in \mathbb{N}\}$$
be a sequence of rvs on $\Omega$ and let $X$ also be an rv on $\Omega$. Then, $X_{n}$ converges uniformly everywhere to $X$ if 
$$\forall \epsilon > 0, \exists N_{\epsilon} \in \mathbb{N} \text{ s.t. } |X_{n}(\omega) - X(\omega)| < \epsilon, \forall n > N_{\epsilon}, \forall \omega \in \Omega.$$ 

\subsection{Definition.} Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let
$$\{X_{n}: n \in \mathbb{N}\}$$
be a sequence of rvs on $\Omega$ and let $X$ also be an rv on $\Omega$. Then, $X_{n}$ converges in probability to $X$ if for all $\delta > 0$, the $n,\delta$-problematic set 
$$A_{n,\delta} = \{\omega \in \Omega: |X_{n}(\omega) - X(\omega)| \geq \delta\}$$
satisfies 
$$P(A_{n,\delta}) \to 0 \text{ as } n \to \infty.$$
We denote this type of convergence as 
$$X_{n} \xrightarrow{P} X.$$

\subsection{Note.} For $\delta_{1} < \delta_{2}$, we have that 
$$A_{n,\delta_{1}} \supseteq A_{n,\delta_{2}}.$$

\subsection{Remark.} Consistency 
$$P\left(|\bar{X}_{n} - \mu| \geq \epsilon\right) \to 0 \text{ as } n \to \infty$$
is a statement about convergence in probability to the degenerate random variable $X = \mu$.

\subsection{Definition.} Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let
$$\{X_{n}: n \in \mathbb{N}\}$$
be a sequence of rvs on $\Omega$ and let $X$ also be an rv on $\Omega$. Then, $X_{n}$ converges in $L^{p}$ to $X$ if
$$\E\left(|X_{n} - X|^{p}\right) \to 0 \text{ as } n \to \infty.$$

\subsection{Remark.} Consider Markov's Inequality 
$$P(|X_{n} - X| \geq \delta) \leq \frac{\E(|X_{n} - X|)}{\delta}.$$
Then, for $p > 0$, we have that 
$$P(|X_{n} - X|^{p} \geq \delta^{p}) \leq \frac{\E(|X_{n} - X|^{p})}{\delta^{p}},$$
so if $X_{n}$ converges in $L^{p}$ to $X$, then $X_{n}$ converges in probability to $X$.

\subsection{Definition.} A cumulative distribution function is a function that satisfies 
\begin{enumerate}
\item[(1)] $F: \mathbb{R} \to [0, 1]$.
\item[(2)] $F$ is non-decreasing.
\item[(3)] $\lim_{x \to -\infty}F(x) = 0$.
\item[(4)] $\lim_{x \to \infty}F(x) = 1$.
\item[(5)] $F$ has left limits.
\item[(6)] $F$ is right-continuous.
\end{enumerate}
If $F$ is continuous, then $F$ is a continuous cumulative distribution function.

\subsection{Definition.} A continuity point of a function $f: \mathbb{R} \to \mathbb{R}$ is a point $t$ such that 
$$\forall \epsilon > 0, \exists \delta > 0 \text{ s.t. if } |x - t| < \delta \text{ then } |f(x) - f(t)| < \epsilon.$$

\subsection{Definition.} A sequence of rvs $X_{n}$ converges in distribution to an rv $X$ if for all continuity points $t$ of $F$, 
$$\lim_{n \to \infty}F_{n}(t) = F(t)$$
where $F_{n}$ is the cdf of $X_{n}$ and $F$ is the cdf of $X$.
We denote this type of convergence as
$$X_{n} \xrightarrow{D} X.$$
Note that $X_{1}, X_{2}, \ldots$ and $X$ need not be defined on a common probability space.

\subsection{Theorem.} Suppose that $g: \mathbb{R} \to \mathbb{R}$ is continuous. Then, if 
$$X_{n} \xrightarrow{D} X,$$
then
$$g(X_{n}) \xrightarrow{D} g(X).$$

\subsection{Lemma.} If a sequence of rvs $X_{n}$ is defined on a common probability space $\Omega$ and 
$$X_{n} \xrightarrow{D} c$$
for some degenerate random variable $c \in \mathbb{R}$, then 
$$X_{n} \xrightarrow{P} c.$$

\subsection{Theorem.} (Slutsky's Theorem.) Suppose that 
$$X_{n} \xrightarrow{D} X, \quad Y_{n} \xrightarrow{D} c$$
for some degenerate random variable $c \in \mathbb{R}$. If $X_{n}, Y_{n}$ are defined on a common $\Omega$ such that $X+Y$ and $XY$ are well-defined, then 
\begin{enumerate}
\item[(1)] $X_{n} + Y_{n} \xrightarrow{D} X + c$.
\item[(2)] $X_{n}Y_{n} \xrightarrow{D} cX$.
\item[(3)] $X_{n}/Y_{n} \xrightarrow{D} X/c$ if $c \neq 0$.
\end{enumerate}

\subsection{Example.} Let $\Omega$ be the unit interval and $P$ be the uniform probability measure. Define the sequence of rvs $X_{n}$ such that 
$$X_{n} = I_{[0, 1/n]}.$$
Define also the degenerate rv $X = 0$.

We claim that $X_{n}$ does not converge pointwise everywhere, does converge pointwise with probability one, does not converge uniformly, does converge uniformly with probability one, does converge in probability, does converge in $L^{2}$, and does converge in distrubtion to $X$.

\textit{Non-Convergence Pointwise Everywhere.} Recall that $X_{n}$ converges pointwise everywhere to $X$ if 
$$X_{n}(\omega) \to X(\omega)$$
for all $\omega \in \Omega$. Consider $\omega = 0$. For all $n \in \mathbb{N}$, $X_{n}(0) = 1$ but $X(0) = 0$, so $X_{n}$ does not converge pointwise everywhere to $X$.

\textit{Convergence Pointwise with Probability One.} Consider the set $A^{c} = \Omega \setminus 0$. Consider an arbitary $\omega \in A^{c}$. Then, for all $\epsilon > 0$, we can choose 
$$N_{\epsilon} = \left\lceil \frac{1}{\omega} \right\rceil + 1$$
such that for all $n > N_{\epsilon}$,
$$|X_{n}(\omega) - 0| = 0.$$
Hence, $X_{n}$ converges pointwise with probability one to $X$.

\textit{Non-Convergence Uniformly.} Recall that $X_{n}$ converges uniformly to $X$ if
$$\forall \epsilon > 0, \exists N_{\epsilon} \in \mathbb{N} \text{ s.t. } |X_{n}(\omega) - X(\omega)| < \epsilon, \forall n > N_{\epsilon}, \forall \omega \in \Omega.$$
Uniform convergence is a stronger condition than pointwise convergence, so if $X_{n}$ does not converge pointwise everywhere to $X$, then $X_{n}$ does not converge uniformly to $X$.

\textit{Convergence in Probability.} Recall that $X_{n}$ converges in probability to $X$ if for all $\delta > 0$, the set
$$A_{n,\delta} = \{\omega \in \Omega: |X_{n}(\omega) - X(\omega)| \geq \delta\}$$
satisfies
$$P(A_{n,\delta}) \to 0 \text{ as } n \to \infty.$$
For any $\delta > 0$, the problematic set is the set for which the indicator is one, i.e., 
$$A_{n,\delta} = [0, 1/n)$$
with probability 
$$P(A_{n,\delta}) = 1/n \to 0 \text{ as } n \to \infty.$$
Hence, $X_{n}$ converges in probability to $X$.

\textit{Convergence in L2.} Recall that $X_{n}$ converges in $L^{2}$ to $X$ if
$$\E(|X_{n} - X|^{2}) \to 0 \text{ as } n \to \infty.$$
We have that 
\begin{align*}
    \E(|X_{n} - X|^{2}) &= \E(I_{[0, 1/n]}) \\
                        &= 1/n \to 0 \text{ as } n \to \infty.
\end{align*}
Hence, $X_{n}$ converges in $L^{2}$ to $X$.

\textit{Convergence in Distribution.} Recall that $X_{n}$ converges in distribution to $X$ if for all continuity points $t$ of $F$, the limit 
$$\lim_{n \to \infty}F_{n}(t) = F(t).$$
The cdf of $X_{n}$ is 
$$F_{n}(t) = \begin{cases} 0 & t < 0 \\ 1 - 1/n & 0 \leq t < 1 \\ 1 & t \geq 1 \end{cases}$$
and the cdf of $X$ is 
$$F(t) = \begin{cases} 0 & t < 0 \\ 1 & t \geq 0 \end{cases}.$$
For all points $t \neq 0$, we have that
$$\lim_{n \to \infty}F_{n}(t) = F(t).$$
But $t = 0$ is not a continuity point of $F$ because 
\begin{align*}
                      F(0) &= 1 \\
    \lim_{t \to 0^{-}}F(t) &= 0.
\end{align*}
Therefore, $X_{n}$ converges in distribution to $X$.

\subsection{Example.} Let $\Omega$ be the unit interval and $P$ be the uniform probability measure. Define the sequence of rvs $X_{n}$ such that 
$$X_{n} = 2^{n}I_{[0, 2^{-n}]}.$$

We claim that $X_{n}$ converges in probability to $X$ but that $X_{n}$ does not converge in $L^{2}$ to $X$.

\textit{Convergence in Probability.} For any $\delta > 0$, the problematic set is the set for which the indicator is one, i.e., 
$$A_{n,\delta} = [0, 2^{-n})$$
with probability
$$P(A_{n,\delta}) = 2^{-n} \to 0 \text{ as } n \to \infty.$$
Hence, $X_{n}$ converges in probability to $X$.

\textit{Non-Convergence in L2.} We have that the second absolute moment is 
\begin{align*}
    \E(|X_{n} - X|^{2}) &= \E(2^{2n}I_{[0, 2^{-n}]}) \\
                        &= 2^{2n}2^{-n} \\
                        &= 2^{n}.
\end{align*}
Therefore, $X_{n}$ does not converge in $L^{2}$ to $X$.

\subsection{Example.} Let $\Omega$ be the unit interval and $P$ be the uniform probability measure. For any $\omega \in \Omega$, let $Y_n(\omega) = g_n(\omega)$, defined as follows. First, for any $n \geq 1$, let $m$ be the largest integer such that $2^m \leq n$. Then note that $n$ can be written uniquely as $n = 2^m + j$, where $0 \leq j < 2^m$. For such $n$, define
$$g_n(\omega) = I_{\left[\frac{j}{2^m}, \frac{j+1}{2^m}\right]}(\omega)$$
where $I$ denotes the indicator function.

We claim that $g_n(\omega)$ converges in probability to zero. But for any value of $\omega \in \Omega$, there exists an infinite sequence of integers $n_k$ where $g_{n_k}(\omega) = 0$ and an infinite sequence of integers $n_l$ where $g_{n_l}(\omega) = 1$, so $g_n(\omega)$ does not converge with probabilty one to zero.

\textit{Convergence in Probability.} For all $\delta \in (0, 1]$, the set for which $|g_{n}(\omega) - 0| \geq \delta$ is $\omega \in [j/2^{m}, (j+1)/2^{m}]$. We thus have that 
\begin{align*}
    P(A_{n, \delta}) &= P\left(\left[\frac{j}{2^m}, \frac{j+1}{2^m}\right]\right) \\ 
                     &= \frac{1}{2^m} \rightarrow 0 \text{ as } n \rightarrow \infty
\end{align*}
where $m$ is defined as the largest integer such that $2^m \leq n$. Observe that the problematic set $A_{n,\delta}$ does not depend on the particular value of $\delta$. So, for all $\delta > 0$, we have that 
$$P(A_{n},\delta) = \frac{1}{2^{m}} = 2^{-\left\lfloor \log_{2}n \right\rfloor}.$$
Suppose that $P(A_{n,\delta})$ converges to $0$. Then, for all $\epsilon > 0$, we can choose 
$$N_{\epsilon} = \left\lceil \frac{1}{\epsilon} \right\rceil + 1$$
such that for all $n > N_{\epsilon}$, 
\begin{align*}
    |2^{-\left\lfloor \log_{2}n \right\rfloor} - 0| &\leq 2^{-\log_{2}n - 1} \\
                                                    &= \frac{1}{2n} \\
                                                    &< \frac{1}{2}\frac{1}{1/\lceil\epsilon\rceil+1} \\
                                                    &< \epsilon.
\end{align*}
Therefore, for all $\delta$, the problematic set $A_{n,\delta}$ converges to probability zero as $n$. Thus, $Y_{n} = g_{n}(\omega)$ converges in probability to zero.

\textit{Non-Convergence with Probability One.} Observe that any number $\omega \in [0, 1]$ can be written in binary form as 
$$\omega = 0.\omega_{1}\omega_{2}\omega_{3}\ldots$$
where $\omega_{i} \in \{0, 1\}$. For all $m$, we truncate $\omega$ at the $m$th digit to get 
$$\omega' = 0.\omega_{1}\omega_{2}\ldots\omega_{m}.$$
For all such $\omega'$, we can write 
$$\omega' = \frac{j}{2^{m}}$$
for some unique $0 \leq j < 2^{m}$. Clearly, then,  
$$\omega \in [j/2^{m}, (j+1)/2^{m}].$$
Thus, for all $m = 1, 2, \ldots$, let $n_{l} = 2^{m} + j$, where $j$ is the unique integer described above. Therefore, 
\begin{align*}
    g_{n_{l}}(\omega) &= I_{\left[\frac{j}{2^{m}}, \frac{j+1}{2^{m}}\right]}(\omega) \\
                      &= 1
\end{align*}
for all $\omega \in \Omega$. Similarly, we can choose $n_{k} = 2^{m} + j - 1$ where $m = m_{1}, m_{2}, ...$ such that $m_{1}$ is the smallest $2^{k}, k \in \mathbb{N}$, such that the special $j$ described above is not $0$. Then, 
\begin{align*}
    g_{n_{k}}(\omega) &= I_{\left[\frac{j-1}{2^{m}}, \frac{j}{2^{m}}\right]}(\omega) \\ 
                      &= 0
\end{align*}
for all $\omega \in \Omega$. Thus, for any value of $\omega \in \Omega$, there exists an infinite sequence of integers $n_{k}$ where $g_{n_{k}}(\omega) = 0$ and an infinite sequence of integers $n_{l}$ where $g_{n_{l}}(\omega) = 1$.

For any $\omega \in [0, 1]$, there exists an unbounded sequence of integers $n_{l}$ where $g_{n_{l}}(\omega) = 1$. Therefore, for any $\omega \in [0, 1]$ and any $\epsilon > 0$, there exists no $N_{\epsilon}$ such that for all $n > N_{\epsilon}$, $|g_{n}(\omega) - 0| < \epsilon$. Thus, $g_{n}(\omega)$ does not converge to zero with probability one.

\newpage \newsection{Parametric Estimation.}

\subsection{Introduction.} Parametric Estimation.

\subsection{Remark.} The motivation for parametric estimation is the following problem. Suppose that we have a collection of iid rvs $X_{n}$ with a common cdf $F_{\theta}$ for some $\theta \in \Theta \subseteq \mathbb{R}^{k}$. Suppose that we know the parametric family of $F_{\theta}$ but not the exact value of $\theta$. Then, we would like to estimate $\theta$ from the observations of $X_{n}$.

\subsection{Remark.} We may ask two types of questions about $\hat{\theta}$. If we investigate finite sample properties of $\hat{\theta}$, then we are interested in the properties of $\hat{\theta}$ for a fixed $n$. Examples of these properties are 
\begin{enumerate}
\item[(1)] Distribution of $\hat{\theta}$
\item[(2)] $\E(\hat{\theta})$
\item[(3)] $\Var(\hat{\theta})$
\end{enumerate}
If we investigate asymptotic properties of $\hat{\theta}$, then we are interested in the properties of $\hat{\theta}$ as $n \to \infty$. Examples of these properties are 
\begin{enumerate}
\item[(1)] Consistency of $\hat{\theta}$ for $\theta$.
\item[(2)] Limiting Distribution of $\hat{\theta}$.
\end{enumerate}

\subsection{Definition.} For an rv $X$ with pdf $f$, the support of $f$ is the set 
$$\text{supp}(f) = \{x \in \mathbb{R}: f(x) > 0\}.$$
For an rv $X$ with a pmf $p$, the support of $p$ is the set
$$\text{supp}(p) = \{x \in \mathbb{R}: p(x) > 0\}.$$

\subsection{Example.} The support of $f_{\theta}$ can depend on $\theta$. Consider $X_{n} \sim \text{unif}(0, \theta)$ for $\theta > 0$. Then, the support of $f_{\theta}$ is
$$\text{supp}(f_{\theta}) = [0, \theta].$$

\subsection{Definition.} Suppose that we have a collection of random variables $X_{n}$ iid with a common cdf $F_{\theta}$ for some $\theta \in \Theta \subseteq \mathbb{R}^{k}$. We may write the parameter $\theta$ as the solution to the system of equations 
$$\mu_{r}(\theta) = \E_{\theta}(X^{r})$$
for $r = 1, 2, \ldots, k$ where $\E(X^{r})$ is the $r$th moment of $X$. We denote the solution to this system of equations as the inverse function 
$$\theta = g(\mu_{1}, \mu_{2}, \ldots, \mu_{k}).$$
Suppose that we do not know the exact value of $\theta$ but we observe a collection of iid rvs $X_{n}$ with a common cdf $F_{\theta}$. Then, we would like to estimate $\theta$ from the observations of $X_{n}$. In particular, 
$$\hat{\theta}_{MOM} = g(\hat{\mu}_{1}, \hat{\mu}_{2}, \ldots, \hat{\mu}_{k})$$
where $\hat{\mu}_{r}$ is the $r$th sample moment of $X_{n}$. We call $\hat{\theta}_{MOM}$ the method of moments (mom) estimator of $\theta$.

\subsection{Example.} If $X_{n} \sim \text{N}(\theta_{1}, \theta_{2})$ iid, then 
\begin{align*}
    \mu_{1} &= \theta_{1} \\
    \mu_{2} &= \theta_{1}^{2} + \theta_{2},
\end{align*}
so 
\begin{align*}
    \hat{\theta}_{1_{MOM}} &= \hat{\mu}_{1} \\
    \hat{\theta}_{2_{MOM}} &= \hat{\mu}_{2} - \hat{\mu}_{1}^{2}.
\end{align*}
Note that this mom estimator is simply the sample mean and sample variance, which is expected for a distribution $X \sim \text{N}(\mu, \sigma^{2})$. Note that $\hat{\theta}_{1_{MOM}} = \bar{X}$ is unbiased for $\theta_{1}$ but that $\hat{\theta}_{2_{MOM}} = \hat{\sigma}^{2}$ is biased for $\theta_{2}$.

\subsection{Example.} Sometimes, the distribution of $\hat{\theta}$ can be determined exactly. For $X_{n} \sim \text{Exp}(\lambda)$ iid, we have that 
$$\hat{\lambda}_{MOM} = \frac{1}{\bar{X}}.$$
But we know that 
$$\bar{X} \sim \text{Gamma}(n, n\lambda).$$

\subsection{Example.} (Continued.) We may also determine the asymptotic properties of $\hat{\lambda}_{MOM} = 1/\bar{X}$. We have that 
$$P\left(\frac{1}{\bar{X}} \leq t\right) = P(\bar{X} \geq \frac{1}{t}).$$
Since $f: t \to 1/t$ is continuous for $t > 0$ and 
$$\bar{X} \xrightarrow{P} \frac{1}{\lambda},$$
then 
$$\hat{\lambda}_{MOM} \xrightarrow{P} \lambda.$$

\subsection{Definition.} For the likelihood function 
$$f(x_{1}, x_{2}, \ldots, x_{n}|\theta),$$
the maximum likelihood estimator (mle) of $\theta$ given the observations $X_{i} = x_{i}$ is the value of $\theta$ that maximizes the likelihood function. That is, 
$$\hat{\theta}_{MLE} = \arg\max_{\theta \in \Theta}f(x_{1}, x_{2}, \ldots, x_{n}|\theta).$$

\subsection{Example.} Suppose that $X_{i}$ are iid. Then, the $\arg\max$ of the likelihood function occurs at 
\begin{align*}
    \arg\max_{\theta \in \Theta}f(x_{1}, x_{2}, \ldots, x_{n}|\theta) &= \arg\max_{\theta \in \Theta}\log f(x_{1}, x_{2}, \ldots, x_{n}|\theta) \\
                                                   &= \arg\max_{\theta \in \Theta}\sum_{i=1}^{n}\log f(x_{i}|\theta)
\end{align*}
since $f: t \to \log t$ is monotone increasing. Under certain conditions, we maximize the likelihood function by setting the gradient of the log-likelihood function equal to zero. That is, setting 
$$\nabla \sum_{i=1}^{n}\log f(x_{i}|\theta) = \left(\frac{\partial}{\partial\theta_{j}}\sum_{i=1}^{n}\log f(x_{i}|\theta)\right)$$
for $j = 1, 2, \ldots k$ equal to zero and solving for $\theta$.

\subsection{Example.} For $X_{i} \sim \text{Exp}(\lambda)$ iid for $\lambda > 0$, the likelihood function is 
$$f(x_{1}, x_{2}, \ldots, x_{n}|\lambda) = \lambda^{n}\exp\left(-\lambda\sum_{i=1}^{n}x_{i}\right).$$
Then, the log-likelihood function is 
$$\log f(x_{1}, x_{2}, \ldots, x_{n}|\lambda) = n\log\lambda - \lambda\sum_{i=1}^{n}x_{i},$$
which is maximized at 
\begin{align*}
          0 &= \frac{n}{\lambda} - \sum_{i=1}^{n}x_{i} \\
    \lambda &= n\left(\sum_{i=1}^{n}x_{i}\right)^{-1}.
\end{align*}
Therefore, 
$$\hat{\lambda}_{MLE} = \frac{1}{\bar{X}}.$$
Note that this agrees with the mom estimator for $\lambda$.

\subsection{Example.} If $X_{n} \sim \text{unif}(0, \theta)$ iid, then 
$$\mu_{1} = \frac{\theta}{2},$$
so
$$\hat{\theta}_{MOM} = 2\hat{\mu}_{1}.$$
But some observations $X_{i}$ may be greater than $\hat{\theta}_{MOM}$, which means that those observations are beyond the support of the estimated $f_{\theta}$.

\subsection{Example.} (Continued.) The likelihood function of $X_{n} \sim \text{unif}(0, \theta)$ iid is 
$$f(x_{1}, x_{2}, \ldots, x_{n}|\theta) = \theta^{-n}I_{\{x_{i} \in [0, \theta] \forall i\}}.$$
Since the support of $f_{\theta}$ depends on $\theta$, then $f(x|\theta)$ is not differentiable in $\theta$, so we cannot set the derivative equal to zero. But we observe that if $\theta \leq \max\{x_{i}\}$, i.e., if $\theta$ is feasible, then 
$$f(x_{1}, x_{2}, \ldots, x_{n}|\theta) = \theta^{-n},$$
which is maximized when $\theta$ is small. Therefore, the mle of $\theta$ is the smallest feasible $\theta$, i.e., 
$$\hat{\theta}_{MLE} = \max\{x_{i}\}.$$
Note that $\hat{\theta}_{MLE}$ is always feasible unlike $\hat{\theta}_{MOM}$.

\subsection{Definition.} The Fisher Information of a random variable $X$ with pdf $f(x|\theta)$ is 
$$I(\theta) = \E\left(\left(\frac{\partial}{\partial\theta}\log f(X|\theta)\right)^{2}\right)$$
where the expected value is taken over the support of $f(x|\theta)$. Under sufficient regularity conditions, the Fisher information is equivalent to 
$$I(\theta) = -\E\left(\frac{\partial^{2}}{\partial\theta^{2}}\log f(X|\theta)\right).$$

\subsection{Theorem.} Suppose that $X_{1}, X_{2}, \ldots, X_{n}$ are iid and $f(x|\theta)$ satisfies sufficient regularities conditions, i.e., 
\begin{enumerate}
\item[(1)] $f(x|\theta)$ is sufficiently regular.
\item[(2)] $\text{supp} f(x|\theta)$ does not depend on $\theta$.
\end{enumerate}
Let $\hat{\theta}_{n}$ be the mle for $\theta \in \Theta$. Then, 
$$\forall \delta > 0, P(|\hat{\theta}_{n} - \theta| \geq \delta) \to 0 \text{ as } n \to \infty,$$
i.e., $\hat{\theta}_{n}$ is consistent for $\theta$, and 
$$\sqrt{n}(\hat{\theta}_{n} - \theta) \xrightarrow{D} \text{N}(0, I(\theta)^{-1})$$
and 
$$\Var(\hat{\theta}_{n}) = \frac{1}{nI(\theta)}.$$

\subsection{Theorem.} (Cramer-Rao Bound.) Suppose that $X_{1}, X_{2}, \ldots, X_{n}$ are iid with common pdf $f(x|\theta)$ that satisfies sufficient regularity conditions. Let $T(X_{1}, X_{2}, \ldots, X_{n})$ be an estimator for $\theta$ and let 
$$\phi(\theta) = \E(T(X_{1}, X_{2}, \ldots, X_{n})).$$
Then, the variance of $T$ satisfies the lower bound 
$$\Var(T) \geq \frac{(\phi'(\theta))^{2}}{nI(\theta)}$$
where $I(\theta)$ is the Fisher Information of $X$ with pdf $f(x|\theta)$.

\subsection{Definition.} Suppose that $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are two estimators for $\theta$. The relative efficiency of $\hat{\theta}_{1}$ with respect to $\hat{\theta}_{2}$ is
$$\text{RE}(\hat{\theta}_{1}, \hat{\theta}_{2}) = \frac{\MSE(\hat{\theta}_{2})}{\MSE(\hat{\theta}_{1})}.$$
If $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are unbiased for $\theta$, then 
$$\text{RE}(\hat{\theta}_{1}, \hat{\theta}_{2}) = \frac{\Var(\hat{\theta}_{2})}{\Var(\hat{\theta}_{1})}.$$

\subsection{Definition.} $T(X_{1}, X_{2}, \ldots, X_{n})$ is sufficient for $\theta$ if the conditional distribution of $X_{1}, X_{2}, \ldots, X_{n}$ given $T(X_{1}, X_{2}, \ldots, X_{n})$ does not depend on $\theta$.

\subsection{Theorem.} (Factorization Theorem.) Suppose that $X_{1}, X_{2}, \ldots, X_{n}$ are iid with common pdf $f(x|\theta)$. $T$ is sufficient for $\theta$ if and only if 
$$f(x_{1}, x_{2}, \ldots, x_{n}|\theta) = g(T(x_{1}, x_{2}, \ldots, x_{n}), \theta)h(x_{1}, x_{2}, \ldots, x_{n}).$$

\subsection{Definition.} $T$ is minimally sufficient for $\theta$ if for all sufficient statistics $S$ for $\theta$, there exists a function $l$ such that 
$$T = l(S).$$

\newpage \newsection{Bayesian Estimation.}

\subsection{Introduction.} Bayesian Estimation.

\subsection{Definition.} Suppose that we have the observations $X_{1}, X_{2}, \ldots, X_{n}$ iid with common pdf $f_{X|\theta}(x|\theta)$. Suppose that we want to estimate a paramter $\theta \in \Theta$ with a prior distribution 
$f_{\theta}(\theta)$. The posterior distribution of $\theta$ given the observations $X_{1}, X_{2}, \ldots, X_{n}$ is
$$f_{\theta|X}(x|\theta) = \frac{f_{X|\theta}(x|\theta)f_{\theta}(\theta)}{f_{X}(x)}$$
where the denominator is a normalizing constant equal to 
$$f_{X}(x) = \int_{\Theta}f_{X|\theta}(x|\theta)f_{\theta}(\theta)\, d\theta.$$

\subsection{Lemma.} Suppose that we have a vector of iid observations $X = (X_{1}, X_{2}, \ldots, X_{n})$ with common pdf $f_{X|\theta}(x|\theta)$. Suppose also that $T(X)$ is a sufficient statistic for $\theta$. Then, the posterior distribution of $\theta$ given $X$ only depends on $T(X)$. For a sketch of the proof, observe that 
\begin{align*}
    f_{\theta|X}(\theta|X) &= \frac{f_{X|\theta}(X|\theta)f_{\theta}(\theta)}{f_{X}(X)} \\
                           &= (g(T, \theta)h(X)f_{\theta}(\theta))\left(\int_{\Theta}g(T, \theta)h(X)f_{\theta}(\theta)\, d\theta\right)^{-1} \\
                           &= g(T, \theta)h(X)\left(\int_{\Theta}g(T, \theta)h(X)f_{\theta}(\theta)\, d\theta\right)^{-1}
\end{align*}
by the factorization theorem.

\subsection{Note.} Suppose that $\theta$ has a distribution of $f(\theta)$. Estimates for $\theta$ include 
\begin{enumerate}
\item[(1)] The mean of $\theta$.
\item[(2)] The median of $\theta$.
\item[(3)] The mode of $\theta$.
\end{enumerate}

\subsection{Definition.} A loss function is $l: \Theta \times A \to \mathbb{R}^{+} \cup 0$ where $A$ is the action space.

\subsection{Definition.} Suppose that we have a vector of observations $(X_{1}, X_{2}, \ldots, X_{n})$. Then, a decision rule is 
$$\delta: (\text{supp} f_{X})^{n} \to A.$$

\subsection{Definition.} Suppose that we have a vector of iid observations $X = (X_{1}, X_{2}, \ldots, X_{n})$ with common pdf $f_{X}(x)$. The risk of a decision rule $\delta$ at parameter value $\theta$ is 
\begin{align*}
    R(\theta, \delta) &= \E(l(\theta, \delta(X))) \\
                      &= \int_{\mathbb{R}^{n}}l(\theta, \delta(x))f_{X}(x)\, dx_{1}dx_{2}\ldots dx_{n}.
\end{align*}

\subsection{Definition.} The Bayes risk of a decision rule $\delta$ is 
\begin{align*}
    r(\delta) &= \E(R(\theta, \delta)) \\
              &= \E(\E(l(\theta, \delta(X))|\theta)) \\
              &= \int_{\Theta}\left[\int_{\mathbb{R}^{n}}l(\theta, \delta(x))f_{X|\theta}(x|\theta)\, dx_{1}dx_{2}\ldots dx_{n}\right]f_{\theta}(\theta)\, d\theta.
\end{align*}

\subsection{Definition.} The Bayes estimator of $\theta$ is the decision rule $\delta$ that minimizes the Bayes risk $r(\delta)$, i.e., 
$$\delta = \arg\min_{\delta\in\Delta}r(\delta).$$

\subsection{Definition.} The minimax estimator of $\theta$ is the decision rule $\delta$ that minimizes the maximum risk, i.e., 
$$\delta = \arg\min_{\delta\in\Delta}\max_{\theta\in\Theta}R(\theta, \delta).$$

\subsection{Definition.} Suppose that our prior $\theta$ belongs to a family $G$ and that our likelihood $f(x|\theta)$ belongs to a family $H$. If the posterior $\theta$ also belongs to $G$, then $G$ and $H$ are said to be conjugate families.

\newpage \newsection{Hypothesis Testing.}

\subsection{Introduction.} Hypothesis Testing.

\subsection{Definition.} Suppose that we are in the frequentist paradigm. Suppose that we observe a vector of iid data $X = (X_{1}, X_{2}, \ldots, X_{n})$ with common pdf $f_{X|\theta}(x|\theta)$. Suppose that we want to test the null hypothesis $H_{0}$ against the alternative hypothesis $H_{1}$ where 
$$H_{0}: \theta \in \Theta_{0} \quad \text{vs.} \quad H_{1}: \theta \in \Theta_{1}.$$
Then, our decision rule is 
$$d: (\text{supp} f_{X})^{n} \to \{0, 1\}$$
where $d(X) = 0$ means that we fail to reject $H_{0}$ and $d(X) = 1$ means that we reject $H_{0}$. Note that the frequentist paradigm assigns probabilities to the data $X$ and not to the hypotheses $H_{0}$ and $H_{1}$.

\subsection{Definition.} Suppose that we are in the Bayesian paradigm. Suppose that we observe a vector of iid data $X = (X_{1}, X_{2}, \ldots, X_{n})$ with common pdf $f_{X|\theta}(x|\theta)$. Suppose that we want to test the null hypothesis $H_{0}$ against the alternative hypothesis $H_{1}$ where
$$H_{0}: f = f_{0} \quad \text{vs.} \quad H_{1}: f = f_{1}.$$
Then, our posterior probabilities are 
\begin{align*}
    P(H_{0}|X) &= \frac{f(x|H_{0})p_{0}}{f(x|H_{0})p_{0} + f(x|H_{1})p_{1}} \\
    P(H_{1}|X) &= \frac{f(x|H_{1})p_{1}}{f(x|H_{0})p_{0} + f(x|H_{1})p_{1}}
\end{align*}
where $p_{0}$ and $p_{1}$, respectively, are the prior probabilities of $H_{0}$ and $H_{1}$. The odds ratio is 
$$\frac{P(H_{0}|X)}{P(H_{1}|X)} = \frac{f(x|H_{0})p_{0}}{f(x|H_{1})p_{1}}.$$
We may say that our decision rule rejects $H_{0}$ if and only if the odds ratio is less than some threshold $c \in \mathbb{R}^{+}$ (often one), i.e., if and only if 
\begin{align*}
    \frac{f(x|H_{0})p_{0}}{f(x|H_{1})p_{1}} &< c \\
                  \frac{f_{0}(x)}{f_{1}(x)} &< c\frac{p_{1}}{p_{0}}.
\end{align*}

\subsection{Definition.} The likelihood ratio test is the decision rule $\delta$ that rejects $H_{0}$ if and only if the likelihood ratio 
$$\text{LR}(X) = \frac{f(X|H_{0})}{f(X|H_{1})}$$
is less than some threshold $c \in \mathbb{R}^{+}$, i.e., if and only if 
$$\frac{f_{0}(x)}{f_{1}(x)} < c.$$

\subsection{Definition.} Type I error is if we reject $H_{0}$ when $H_{0}$ is true. Type II error is if we fail to reject $H_{0}$ when $H_{0}$ is false.

\subsection{Definition.} The Type I error probability, i.e., the significance level $\alpha$, for a decision rule $\delta$ is 
$$\alpha = P(\delta(X)=1|H_{0})$$
if $H_{0} \cap H_{1} = \emptyset$ and $H_{0} \cup H_{1} = H$ where $H$ is the hypothesis space.

\subsection{Definition.} The power of a decision rule $\delta$ is 
$$\text{Power}(\delta) = P(\delta(X)=1|H_{1})$$
where $H_{0} \cap H_{1} = \emptyset$ and $H_{0} \cup H_{1} = \mathcal{H}$ where $\mathcal{H}$ is the hypothesis space.

\subsection{Definition.} The $p$-value for an observed statistic $T_{0}$ is the smallest $\alpha$ for which we reject $H_{0}$ given $T_{0}$ at significance level $\alpha$ with the likelihood ratio test.

\subsection{Definition.} A hypothesis $H$ is simple if it completely specifies the distribution from which the data are taken, i.e., if the the hypothesis space under $H$, i.e., $\mathcal{H}_{0}$ is a singleton set. If $H$ is not simple, then it is composite.

\subsection{Lemma.} (Neyman-Pearson Lemma.) Suppose that we have a vector of iid data $X = (X_{1}, X_{2}, \ldots, X_{n})$ with common pdf $f_{X|\theta}(x|\theta)$. Suppose that we want to test the null hypothesis $H_{0}$ against the alternative hypothesis $H_{1}$ where 
$$H_{0}: \theta = \theta_{0} \quad \text{vs.} \quad H_{1}: \theta = \theta_{1}$$
such that $H_{0}, H_{1}$ are simple and that $\theta_{0} \neq \theta_{1}$ and that $H_{0} \cup H_{1} = \mathcal{H}$. Then, the likelihood ratio test $\delta$ is the most powerful test for $H_{0}$ against $H_{1}$ at significance level $\alpha$. 

That is, for any test $\delta^{*}$ with significance level $\alpha^{*} \leq \alpha$, i.e, with 
$$P(\delta^{*}(X)=1|H_{0}) \leq \alpha,$$
then 
$$P(\delta^{*}(X)=1|H_{1}) \leq P(\delta(X)=1|H_{1}).$$
If $\alpha^{*} < \alpha$, then the above inequality is strict.

\subsection{Definition.} Suppose that we have the hypotheses 
$$H_{0}: \theta = \theta_{0} \quad \text{vs.} \quad H_{1}: \theta \in \Theta_{1}.$$
A test $\delta$ is uniformly most powerful for the composite hypothesis $H_{1}$ if for any $\theta_{1} \in \Theta_{1}$, the test $\delta$ is most powerful for the hypotheses  
$$H_{0}: \theta = \theta_{0} \quad \text{vs.} \quad H_{1}: \theta = \theta_{1}.$$

\subsection{Definition.} Suppose that we observe a vector of iid data $X = (X_{1}, X_{2}, \ldots, X_{n})$ with common pdf $f_{X|\theta}(x|\theta)$. Suppose that we want to test the null hypothesis $H_{0}$ against the alternative hypothesis $H_{1}$ where 
$$H_{0}: \theta = \Theta_{0} \quad \text{vs.} \quad H_{1}: \theta \not\in \Theta_{0}.$$
The generalized likelihood ratio (glr) is 
$$\Lambda = \max_{\theta \in \Theta_{0}}f(x|\theta)(\max_{\theta \in \Theta}f(x|\theta))^{-1}.$$
The glr test is to reject $H_{0}$ if and only if $\Lambda < c$ where $c \in \mathbb{R}^{+}$.

\subsection{Lemma.} Suppose that $X_{1}, X_{2}, \ldots, X_{n} \sim \text{N}(\mu, \sigma^{2})$ iid. Then, the sample mean $\bar{X}$ is independent of the vector $W = (X_{1} - \bar{X}, X_{2} - \bar{X}, \ldots, X_{n} - \bar{X})$.

\subsection{Corollary.} (Continued.) If $X_{1}, X_{2}, \ldots, X_{n} \sim \text{N}(\mu, \sigma^{2})$ iid, then the sample mean $\bar{X}$ is independent of the sample variance 
$$S^{2} = \frac{1}{n-1}||W||_{2}^{2}.$$

\subsection{Definition.} A rv $W$ has a chi-square distribution with $\nu$ df if $W$ has a Gamma distribution with shape parameter $\nu/2$ and rate parameter $1/2$, i.e., $W \sim \chi_{\nu}^{2}$ if $W \sim \text{Gamma}(\nu/2, 1/2)$ with pdf 
$$f(w) = \frac{1}{2^{\nu/2}\Gamma(\nu/2)}w^{\nu/2-1}e^{-w/2}$$
for $w > 0$.

\subsection{Lemma.} If $Z \sim \text{N}(0, 1)$, then $Z^{2} \sim \chi_{1}^{2}$.

\subsection{Lemma.} If $W_{1} \sim \chi_{\nu_{1}}^{2}$ and $W_{2} \sim \chi_{\nu_{2}}^{2}$, then $W_{1} + W_{2} \sim \chi_{\nu_{1} + \nu_{2}}^{2}$.

\subsection{Corollary.} (Continued.) If $Z_{1}, Z_{2}, \ldots, Z_{n} \sim \text{N}(0, 1)$ iid, then 
$$\sum_{i=1}^{n}Z_{i}^{2} \sim \chi_{n}^{2}.$$

\subsection{Corollary.} If $X_{1}, X_{2}, \ldots, X_{n} \sim \text{N}(\mu, \sigma^{2})$ iid, then
$$\frac{(n-1)S^{2}}{\sigma^{2}} \sim \chi_{n-1}^{2}.$$

\subsection{Definition.} $T$ has a Student's $\text{t}$ distribution with $\nu$ df $t(\nu)$ if $T$ has the pdf
$$f(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\nu\pi}\Gamma(\nu/2)}\left(1 + \frac{t^{2}}{\nu}\right)^{-(\nu+1)/2}.$$

\subsection{Lemma.} Suppose that $Z \sim \text{N}(0, 1)$ and $V \sim \chi_{\nu}^{2}$ are independent. Then, 
$$T = \frac{Z}{\sqrt{V/\nu}} \sim \text{t}_{\nu}.$$

\subsection{Lemma.} Suppose that $X_{1}, X_{2}, \ldots, X_{2} \sim \text{N}(\mu, \sigma^{2})$ iid. Then, 
$$\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}.$$

\subsection{Proof.} (Continued.) Observe that 
\begin{align*}
    \frac{\bar{X}-\mu}{S/\sqrt{n}} &= \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}/\frac{S/\sqrt{n}}{\sigma/\sqrt{n}} \\
                                   &= \frac{Z}{s/\sigma} \\
                                   &= Z/\sqrt{\frac{s^{2}(n-1)}{\sigma^{2}(n-1)}} \\
                                   &= \frac{Z}{\sqrt{V/(n-1)}} \sim t_{n-1}
\end{align*}
because we know from Corollary (13.13) that $\bar{X}$ is independent of $S^{2}$ and therefore $Z$ is independent of $V$.

\subsection{Definition.} $X$ hsa an $\text{F}$ distribution with $\nu_{1}$ and $\nu_{2}$ df if $X$ has the pdf 
$$f(x) = \frac{\Gamma((\nu_{1}+\nu_{2})/2)}{\Gamma(\nu_{1}/2)\Gamma(\nu_{2}/2)}\left(\frac{\nu_{1}}{\nu_{2}}\right)^{\nu_{1}/2}\frac{x^{\nu_{1}/2-1}}{(1 + \nu_{1}x/\nu_{2})^{(\nu_{1}+\nu_{2})/2}}.$$

\subsection{Lemma.} If $V_{1} \sim \chi_{\nu_{1}}^{2}$ and $V_{2} \sim \chi_{\nu_{2}}^{2}$ are independent, then
$$\frac{V_{1}/\nu_{1}}{V_{2}/\nu_{2}} \sim \text{F}_{\nu_{1}, \nu_{2}}.$$
If $Y \sim \text{F}_{\nu_{1}, \nu_{2}}$, then 
$$1/Y \sim \text{F}_{\nu_{2}, \nu_{1}}.$$
If $T \sim \text{t}{\nu}$, then 
$$T^{2} \sim \text{F}_{1, \nu}.$$
because 
$$\frac{Z^{2}}{V/\nu} \sim \text{F}_{1, \nu}.$$

\subsection{Example.} Suppose that $X_{1}, X_{2}, \ldots, X_{n} \sim \text{N}(\mu, \sigma^{2})$ iid and that we have the hypotheses 
$$H_{o}: \mu = \mu_{0}, \sigma^{2} > 0 \quad \text{vs.} \quad H_{1}: \mu \neq \mu_{0}, \sigma^{2} > 0.$$
Then, we have that the glr is 
$$\Lambda = \frac{f(x|\mu_{0}, \hat{\sigma}_{0}^{2})}{f(x|\hat{\mu}, \hat{\sigma}^{2})}$$
where 
$$\hat{\mu}_{0} = \mu_{0}, \quad \hat{\sigma}_{0}^{2} = \frac{1}{n}\sum_{i=1}^{n}(x_{i} - \mu_{0})^{2}$$
$$\hat{\mu} = \bar{x}, \quad \hat{\sigma}^{2} = \frac{1}{n}\sum_{i=1}^{n}(x_{i} - \bar{x})^{2}.$$
The likelihood ratio simplifies to 
$$\Lambda = \left(\frac{\hat{\sigma}}{\hat{\sigma}_{0}}\right)^{n}.$$
So we reject $H_{0}$ if the ratio $\hat{\sigma}^{2}_{0}/\hat{\sigma}^{2} > c$. Observe that 
$$\sum_{i=1}^{n}(x_{i}-\mu_{0})^{2} / \sum_{i=1}^{n}(x_{i}-\bar{x})^{2} = 1 + n(\bar{x} - \mu_{0})^{2}/\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}.$$
So we reject $H_{0}$ if the ratio
$$n(\bar{x} - \mu_{0})^{2}/\sum_{i=1}^{n}(x_{i}-\bar{x})^{2} > c.$$
But observe that 
$$n(n-1)(\bar{X} - \mu_{0})^{2}/\sum_{i=1}^{n}(X_{i} - \bar{X})^{2} = \left(\frac{\bar{X}-\mu_{0}}{\sigma/\sqrt{n}}\right)^{2} / \frac{S^{2}(n-1)}{\sigma^{2}(n-1)} \sim F_{1, n-1}.$$
Define the test statistic $W$ as 
$$W = \left(\frac{\bar{X}-\mu_{0}}{\sigma/\sqrt{n}}\right)^{2} / \frac{S^{2}(n-1)}{\sigma^{2}(n-1)}.$$
Then, we reject $H_{0}$ if $W > c$ where $c$ is the $1-\alpha$ quantile of the $F_{1, n-1}$ distribution. Observe also that 
$$T = \sqrt{W} \sim t_{n-1},$$
so we reject $H_{0}$ if $|T| > c$ where $c$ is the $1-\alpha/2$ quantile of the $t_{n-1}$ distribution.

\subsection{Theorem.} (Wilks's Theorem.) Suppose that we have a vector of iid data $X = (X_{1}, X_{2}, \ldots, X_{n})$ with common pdf $f_{X|\theta}(x|\theta)$. Suppose that $f(x|\theta)$ satisfies sufficient regularity conditions. Suppose that
we have the hypotheses 
$$H_{0}: \theta = \theta_{0} \quad \text{vs.} \quad H_{1}: \theta \neq \theta_{0}$$
and that 
$$\dim \mathcal{H} - \dim \mathcal{H}_{0} = k \in \mathbb{N}.$$
Then, 
$$-2\log\Lambda \xrightarrow{D} \chi_{k}^{2} \text{ as } n \to \infty$$
where $\Lambda$ is the glr.

\subsection{Definition.} Suppose that we have $I$ vectors of $n_{i}$ data such that 
$$Y_{ij} \sim \text{N}(\mu_{i}, \sigma^{2})$$
for $i = 1, 2, \ldots, I$ and $j = 1, 2, \ldots, n_{i}$. Define 
$$\mu = \frac{1}{I}\sum_{i=1}^{I}\mu_{i}$$
and 
$$\alpha_{i} = \mu_{i} - \mu$$
such that $\alpha_{1} + \alpha_{2} + \ldots + \alpha_{I} = 0$. We have then that the data $Y_{ij}$ can be written as
$$Y_{ij} \sim \text{N}(\mu + \alpha_{i}, \sigma^{2})$$
such that 
$$Y_{ij} = \alpha_{i} + \mu + X_{i}$$
where $X_{ij} \sim \text{N}(0, \sigma^{2})$ iid. Suppose that we don't know the value of $\sigma^{2}$ but we know that it is the same for all $I$ vectors. Then, the likelihood of our data is 
\begin{align*}
    f(y|\mu, \alpha, \sigma^{2}) &= \prod_{i=1}^{I}\prod_{j=1}^{n_{i}}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{1}{2\sigma^{2}}(y_{ij} - (\alpha_{i} + \mu))^{2}\right) \\
                                 &= \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^{N}\exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{I}\sum_{j=1}^{n_{i}}(y_{ij} - (\alpha_{i} + \mu))^{2}\right)
\end{align*}
where $N = n_{1} + n_{2} + \ldots + n_{I}$. Notice that our degrees of freedom are $\mu, \sigma^{2}, \alpha_{1}, \alpha_{2}, \ldots, \alpha_{I-1}$ because $\alpha_{I}$ is uniquely determined by the other $\alpha_{i}$.

\subsection{Example.} Suppose that we have two vectors $Y_{1\cdot}$ and $Y_{2\cdot}$ such that 
\begin{align*}
    Y_{11}, Y_{12}, \ldots, Y_{1n_{1}} &\sim \text{N}(\mu_{1}, \sigma^{2}) \\
    Y_{21}, Y_{22}, \ldots, Y_{2n_{2}} &\sim \text{N}(\mu_{2}, \sigma^{2})
\end{align*}
where all observations are independent. Suppose that $\sigma^{2}$ is known and that we have the hypotheses 
$$H_{0}: \mu_{1} = \mu_{2} \quad \text{vs.} \quad H_{1}: \mu_{1} \neq \mu_{2}.$$
Note that $H_{0}$ is not simple because the set of all possible values of $\mu_{1} = \mu_{2}$ is of dimension one. We have that the glr is 
$$\Lambda = \frac{f(y|\mu_{1}, \mu_{2}, \sigma^{2})}{f(y|\mu, \mu, \sigma^{2})}.$$
Under $H_{0}$, 
$$\hat{\mu} = \frac{n_{1}\bar{Y}_{1\cdot} + n_{2}\bar{Y}_{2\cdot}}{n_{1} + n_{2}}$$
and under $H_{1}$, our maximizer is 
$$(\hat{\mu}_{1}, \hat{\mu}_{2}) = (\bar{Y}_{1\cdot}, \bar{Y}_{2\cdot}).$$
We have then that the glr is 
$$\Lambda = \exp\left(-\frac{1}{2\sigma^{2}}\left(\sum_{j=1}^{n_{1}}(Y_{1j} - \hat{\mu})^{2} + \sum_{j=1}^{n_{2}}(Y_{2j} - \hat{\mu})^{2}\right)\right) / \exp\left(-\frac{1}{2\sigma^{2}}\left(\sum_{j=1}^{n_{1}}(Y_{1j} - \hat{\mu}_{1})^{2} + \sum_{j=1}^{n_{2}}(Y_{2j} - \hat{\mu}_{2})^{2}\right)\right).$$
We reject $H_{0}$ if $\Lambda < c$, so we reject $H_{0}$ if $\log\Lambda < c$, i.e., if 
$$\sum_{j=1}^{n_{1}}(Y_{1j} - \hat{\mu}_{1})^{2} + \sum_{j=1}^{n_{2}}(Y_{2j} - \hat{\mu}_{2})^{2} - \sum_{j=1}^{n_{1}}(Y_{1j} - \hat{\mu})^{2} - \sum_{j=1}^{n_{2}}(Y_{2j} - \hat{\mu})^{2} < c.$$
But observe that 
$$\sum_{j=1}^{n_{1}}(Y_{1j} - \hat{\mu}_{1} + \hat{\mu}_{1} - \hat{\mu})^{2} = \sum_{j=1}^{n_{1}}(Y_{1j} - \hat{\mu}_{1})^{2} + n_{1}(\hat{\mu}_{1} - \hat{\mu})^{2}$$
and similarly for $Y_{2\cdot}$. It follows then that we reject $H_{0}$ if 
$$n_{1}(\hat{\mu}_{1} - \hat{\mu})^{2} + n_{2}(\hat{\mu}_{2} - \hat{\mu})^{2} > c.$$

\end{document}